{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CusfguIfItk9"
      ],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 4140,
          "sourceType": "datasetVersion",
          "datasetId": 2477
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trcamnguyen/Logistic-Regression/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from scipy.special import softmax as sftmx #used to test my function, allowed\n",
        "# fixing random seed for reproducibility\n",
        "random.seed(123)\n",
        "np.random.seed(123)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:31:36.292691Z",
          "start_time": "2020-02-15T14:31:35.549108Z"
        },
        "id": "tfe_F1IAItkk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:23:58.691471Z",
          "iopub.execute_input": "2025-10-02T12:23:58.692180Z",
          "iopub.status.idle": "2025-10-02T12:23:58.698008Z",
          "shell.execute_reply.started": "2025-10-02T12:23:58.692151Z",
          "shell.execute_reply": "2025-10-02T12:23:58.696870Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Raw texts and labels into arrays\n",
        "\n",
        "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
      ],
      "metadata": {
        "id": "OdgfwX6pItkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "# Download Sentiment140\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "file_path = path + \"/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "cols = ['target','id','date','flag','user','text']\n",
        "df = pd.read_csv(file_path, encoding='ISO-8859-1', names=cols)\n",
        "\n",
        "# Giá»¯ láº¡i text + label, Ä‘á»•i 4 -> 1\n",
        "df = df[['text','target']]\n",
        "df['target'] = df['target'].replace(4,1)\n",
        "\n",
        "# Láº¥y subset nhá» gá»n 10,000 dÃ²ng\n",
        "df_small = df.sample(n=10000, random_state=42)\n",
        "\n",
        "# Chia train/dev/test\n",
        "train_df, test_df = train_test_split(df_small, test_size=0.2, random_state=42, stratify=df_small['target'])\n",
        "train_df, dev_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['target'])\n",
        "\n",
        "# Format\n",
        "data_tr_textlist = train_df[['text']].values.tolist()\n",
        "ArrayTR = train_df[['target']].to_numpy()\n",
        "\n",
        "data_dev_textlist = dev_df[['text']].values.tolist()\n",
        "ArrayDev = dev_df[['target']].to_numpy()\n",
        "\n",
        "data_test_textlist = test_df[['text']].values.tolist()\n",
        "ArrayTest = test_df[['target']].to_numpy()\n",
        "\n",
        "print(\"Binary dataset size:\", len(df_small))\n",
        "print(\"Train/Dev/Test:\", len(data_tr_textlist), len(data_dev_textlist), len(data_test_textlist))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyd1BBp1Itkp",
        "outputId": "fa72b383-a592-49b1-952a-21166663ea8a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:02.361903Z",
          "iopub.execute_input": "2025-10-02T12:24:02.362768Z",
          "iopub.status.idle": "2025-10-02T12:24:07.176997Z",
          "shell.execute_reply.started": "2025-10-02T12:24:02.362738Z",
          "shell.execute_reply": "2025-10-02T12:24:07.176011Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Binary dataset size: 10000\nTrain/Dev/Test: 7200 800 2000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag-of-Words Representation\n",
        "\n",
        "\n",
        "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
        "\n",
        "\n",
        "## Text Pre-Processing Pipeline\n",
        "\n",
        "To obtain a vocabulary of features, you should:\n",
        "- tokenise all texts into a list of unigrams (tip: using a regular expression)\n",
        "- remove stop words (using the one provided or one of your preference)\n",
        "- compute bigrams, trigrams given the remaining unigrams\n",
        "- remove ngrams appearing in less than K documents\n",
        "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (you can keep top N if you encounter memory issues).\n"
      ],
      "metadata": {
        "id": "OmktummkItkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = ['a','in','on','at','and','or',\n",
        "              'to', 'the', 'of', 'an', 'by',\n",
        "              'as', 'is', 'was', 'were', 'been', 'be',\n",
        "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
        "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
        "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
        "             'his', 'her', 'they', 'them', 'from', 'with', 'its','also','so','there','their','The']"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:31.860420Z",
          "start_time": "2020-02-15T14:17:31.855439Z"
        },
        "id": "HRspqqpRItkt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:09.227122Z",
          "iopub.execute_input": "2025-10-02T12:24:09.227443Z",
          "iopub.status.idle": "2025-10-02T12:24:09.234062Z",
          "shell.execute_reply.started": "2025-10-02T12:24:09.227418Z",
          "shell.execute_reply": "2025-10-02T12:24:09.232544Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gram extraction from a document\n",
        "\n",
        "You first need to implement the `extract_ngrams` function. It takes as input:\n",
        "- `x_raw`: a string corresponding to the raw text of a document\n",
        "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
        "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
        "- `stop_words`: a list of stop words\n",
        "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
        "\n",
        "and returns:\n",
        "\n",
        "- a list of all extracted features.\n",
        "\n",
        "See the examples below to see how this function should work."
      ],
      "metadata": {
        "id": "lUOnMJC_Itkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', stop_words= stop_words, vocab=set()):\n",
        "\n",
        "\n",
        "    ####Tokenisation#######\n",
        "    com = re.compile(token_pattern)\n",
        "    x_raw = com.findall(x_raw)\n",
        "    # vocab = re.findall(token_pattern,vocab)\n",
        "\n",
        "   # x_raw = list(x_raw.split())     #Split Sentence into seperate words then store as list\n",
        "\n",
        "\n",
        " #####Remove Stop words########\n",
        "    x_raw=[word for word in x_raw if word not in stop_words]\n",
        "    for word in stop_words:\n",
        "        for word2 in x_raw:\n",
        "            if word == word2:\n",
        "                x_raw.remove(word)\n",
        "\n",
        "\n",
        "\n",
        "    #print(vocab)\n",
        "\n",
        "#######Return Vocab#####\n",
        "    #vocab needs to normalised\n",
        "    vocab1 = str(vocab)\n",
        "    vocab1 = re.findall(token_pattern,vocab1)\n",
        "    commachar = \",\"\n",
        "    spacechar = ''\n",
        "    if ((len(vocab1)) != 0): #Check if set is empty\n",
        "        vocablist = []\n",
        "        for word in x_raw:\n",
        "            for word2 in vocab1:\n",
        "                if ((word == word2) & (word != commachar) & (word != spacechar)& (word2 not in vocablist)): #remove commas & match words in text &vocab\n",
        "                    vocablist.append(word.replace(\" \", \"\")) ## remove whitespace characters\n",
        "                   # print(vocablist)\n",
        "\n",
        "    noofngrams=[]\n",
        "    ngrams_list = []\n",
        "    if ngram_range == (1,3):\n",
        "        noofngrams= [1,2,3]\n",
        "    if ngram_range == (1,2):\n",
        "        noofngrams= [1,2]\n",
        "\n",
        "    y =[]\n",
        "\n",
        "    #Extract Ngrams\n",
        "    for n in noofngrams:\n",
        "        if (len(vocab)) == 0:\n",
        "                for num in range(0, len(x_raw)):\n",
        "                    ngram = ' '.join(x_raw[num:num + n])\n",
        "                    #if ngram not in ngrams_list:\n",
        "                    if ((ngram != '')):\n",
        "                        ngrams_list.append(ngram)\n",
        "                        if ngram not in y:\n",
        "                            y.append(ngram)\n",
        "\n",
        "        if (len(vocab)) != 0:\n",
        "                for n in range(0,len(vocab1)):\n",
        "                    for num in range(0, len(vocablist)):\n",
        "                        if len(vocablist[num]) != 0:\n",
        "                                ngram = ' '.join(vocablist[num:num + n])\n",
        "\n",
        "                                if ((ngram != '')): # ((ngram not in ngrams_list) &\n",
        "                                    ngrams_list.append(ngram)\n",
        "\n",
        "                                    if ngram not in y:\n",
        "                                        y.append(ngram)\n",
        "\n",
        "    x= ngrams_list\n",
        "    return x,y # y is unique ngram list, x is list including duplicates for vectorisation"
      ],
      "metadata": {
        "id": "CrI3-VziItku",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:11.865374Z",
          "iopub.execute_input": "2025-10-02T12:24:11.865715Z",
          "iopub.status.idle": "2025-10-02T12:24:11.877557Z",
          "shell.execute_reply.started": "2025-10-02T12:24:11.865692Z",
          "shell.execute_reply": "2025-10-02T12:24:11.876617Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "x1,y1 = extract_ngrams(\"this is a great movie to watch\",\n",
        "               ngram_range=(1,3),\n",
        "               stop_words=stop_words)\n",
        "print(y1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:33.680114Z",
          "start_time": "2020-02-15T14:17:33.675339Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmbRpQasItkw",
        "outputId": "c4cb0e7b-51f9-4017-901a-8e1ea0bf547b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:14.774636Z",
          "iopub.execute_input": "2025-10-02T12:24:14.774969Z",
          "iopub.status.idle": "2025-10-02T12:24:14.780949Z",
          "shell.execute_reply.started": "2025-10-02T12:24:14.774949Z",
          "shell.execute_reply": "2025-10-02T12:24:14.779919Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['great', 'movie', 'watch', 'great movie', 'movie watch', 'great movie watch']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
      ],
      "metadata": {
        "id": "jWYnA9xOItky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a vocabulary of n-grams\n",
        "\n",
        "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
        "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
        "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
        "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
        "- `stop_words`: a list of stop words\n",
        "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
        "- `min_df`: keep ngrams with a minimum document frequency.\n",
        "- `keep_topN`: keep top-N more frequent ngrams.\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `vocab`: a set of the n-grams that will be used as features.\n",
        "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
        "- `ngram_counts`: counts of each ngram in vocab\n",
        "\n",
        "Hint: it should make use of the `extract_ngrams` function."
      ],
      "metadata": {
        "id": "8un6kwCMItkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=0, stop_words=[]):\n",
        "\n",
        "    #Define empty placeholder variables\n",
        "    df=  Counter()\n",
        "    ngram_counts = Counter()\n",
        "    vocab = set()\n",
        "    df1 = list()\n",
        "    df2 = Counter()\n",
        "    #com = re.compile(token_pattern)\n",
        "    #X_raw = com.findall(X_raw)\n",
        "    vocab1 = set()\n",
        "\n",
        "    for x_raw in X_raw:\n",
        "        x_raw = str(x_raw)\n",
        "        com = re.compile(token_pattern)\n",
        "        x_raw = com.findall(x_raw)\n",
        "        x_raw = str(x_raw)\n",
        "        x_raw.replace(\" \", \"\")\n",
        "        x_raw.replace(\"[\",\"\")\n",
        "        x_raw.replace(\"]\",\"\")\n",
        "        y,x = extract_ngrams(x_raw, ngram_range, token_pattern, stop_words)\n",
        "        #calling extract_ngrams deals with tokenisation,stop words,ngram range and ngram extraction\n",
        "        for item in x:\n",
        "            df[item] += 1\n",
        "            if df[item]> min_df: # Only keep above the minimum\n",
        "                df2[item] += 1\n",
        "\n",
        "            if (str(item) not in vocab) & (len(vocab) < keep_topN): #stop adding to vocab once cap is reached\n",
        "            #vocab.add(str(x))\n",
        "                 vocab.add(str(item)) #need to remove brackets as i think its passing\n",
        "\n",
        "        for string in x:\n",
        "            if string in ngram_counts:\n",
        "                    ngram_counts[string] += 1 #len 998359\n",
        "            else:\n",
        "                    ngram_counts[string] = 1\n",
        "    #df1.append(df2.most_common(keep_topN)) #Keep only the specified amount of N\n",
        "    #ngram_counts.append(ngram_counts.most_common(keep_topN))\n",
        "    #vocab = list(vocab)\n",
        "    df = df2\n",
        "\n",
        "\n",
        "#######assign top ngrams to vocab set###########################\n",
        "   # vocab1 = set()\n",
        "    for item in ngram_counts.most_common(keep_topN):\n",
        "\n",
        "            # type(item)) item is a tupple\n",
        "        if (len(vocab1) < keep_topN)  :\n",
        "                vocab1.add(item[0])\n",
        "\n",
        "    vocab = vocab1\n",
        "    return vocab, df, ngram_counts"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:35.821240Z",
          "start_time": "2020-02-15T14:17:35.814722Z"
        },
        "id": "nfhxA8jEItk0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:17.325291Z",
          "iopub.execute_input": "2025-10-02T12:24:17.325640Z",
          "iopub.status.idle": "2025-10-02T12:24:17.334305Z",
          "shell.execute_reply.started": "2025-10-02T12:24:17.325606Z",
          "shell.execute_reply": "2025-10-02T12:24:17.333276Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
      ],
      "metadata": {
        "id": "dYwdXNWZItk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, df, ngram_counts = get_vocab(data_tr_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
        "print(len(vocab))\n",
        "print()\n",
        "print(list(vocab)[:100])\n",
        "print()\n",
        "print(df.most_common()[:10])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:39.319793Z",
          "start_time": "2020-02-15T14:17:36.836545Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BCE_lc-Itk1",
        "outputId": "6731f72c-8bcd-4357-deef-b4a743ffbda2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:20.217581Z",
          "iopub.execute_input": "2025-10-02T12:24:20.217925Z",
          "iopub.status.idle": "2025-10-02T12:24:21.221749Z",
          "shell.execute_reply.started": "2025-10-02T12:24:20.217902Z",
          "shell.execute_reply": "2025-10-02T12:24:21.220789Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "5000\n\n['threads', 'employees', 'quiet', 'HELP', 'us all', 'new iphone', 'blame', 'even though', 'Done', 'He', 'since not', 'saturday', 'meet', 'think thats', 'OF', 'wrenching', 'but idk', 'failing', 'okay', 'sister', 'Carly', 'ew', 'Working', 'Mother', 'sleep but', 'whats next', 'am just', 'ministry', 'hasnt', 'like go', 'massive', 'wonder if', 'my name', 'my computer', 'hi', 'combination', 'Just got', 'Come back', 'work all', 'emailed', 'Heading', 'if got', 'hours', 'know just', 'forever', 'my neck', 'after hour', 'had much', 'boyfriend', 'single', 'ON', 'non', 'myspace', 'ice cream', 'gay', 'home work', 'wana', 'sweet dreams', 'yr', 'emily', 'NEED', 'last night', 'Pink', 'might', 'exams', 'weeks till', 'York', 'magazine', 'drama', 'launched', 'Lines Vines Trying', 'Trying get', 'wondering', 'im sad', 'bad', 'pink', 'Youtube', 'default', 'America', 'heat', 'lol not my', 'Hope', 'Netherlands', 'pony', 'last few', 'heart', 'Have good', 'me either', 'blessed', 'voice', 'club', 'Thinking', 'go back sleep', 'spanish', 'answer me', 're back', 'shut up', 'uk', 'com Once', 'well just']\n\n[('my', 1124), ('me', 640), ('but', 499), ('not', 419), ('just', 416), ('up', 378), ('get', 340), ('good', 339), ('now', 332), ('all', 332)]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, you need to create vocabulary id -> word and id -> word dictionaries for reference:"
      ],
      "metadata": {
        "id": "aDUK6EtxItk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_vocab = list(vocab)\n",
        "vocab_id = {i:list_of_vocab[i] for i in range(len(vocab))}"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:39.326811Z",
          "start_time": "2020-02-15T14:17:39.322256Z"
        },
        "id": "-eU1uRMgItk2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:23.668870Z",
          "iopub.execute_input": "2025-10-02T12:24:23.669167Z",
          "iopub.status.idle": "2025-10-02T12:24:23.676168Z",
          "shell.execute_reply.started": "2025-10-02T12:24:23.669145Z",
          "shell.execute_reply": "2025-10-02T12:24:23.675225Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you should be able to extract n-grams for each text in the training, development and test sets:"
      ],
      "metadata": {
        "id": "i6LEY9FnItk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorise documents"
      ],
      "metadata": {
        "id": "islhfNHEItk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
        "- `X_ngram`: a list of texts (documents), where each text (doc) is represented as list of n-grams in the `vocab`\n",
        "- `vocab`: a set of n-grams to be used for representing the documents\n",
        "\n",
        "and return:\n",
        "- `X_vec`: an array with dimensionality N x |vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
      ],
      "metadata": {
        "id": "Mw8BuNP2Itk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Take training set as input\n",
        "#divide into doc\n",
        "#extract each ngram for each doc\n",
        "#Count ngram in doc using counter\n",
        "#iterate counter through the vocab and assign counts in that order\n",
        "def Extract_X_ngram(data_tr_textlist):\n",
        "    X_ngram =[]\n",
        "    for line in data_tr_textlist:\n",
        "        ngram,p = list(extract_ngrams(str(line),stop_words= stop_words))\n",
        "        X_ngram.append(ngram)\n",
        "    return X_ngram\n",
        "\n",
        "X_ngram = Extract_X_ngram(data_tr_textlist)\n",
        "#print(Counter(X_ngram[0]))"
      ],
      "metadata": {
        "id": "c9qF2QbIItk4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:25.840387Z",
          "iopub.execute_input": "2025-10-02T12:24:25.840766Z",
          "iopub.status.idle": "2025-10-02T12:24:26.191870Z",
          "shell.execute_reply.started": "2025-10-02T12:24:25.840728Z",
          "shell.execute_reply": "2025-10-02T12:24:26.190963Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorise(X_ngram, vocab):\n",
        "    N = len(X_ngram) #Get Dimensions #1400 x 5000 for training set\n",
        "    SizeOfSet = len(vocab)\n",
        "    X_vec = np.zeros((N,SizeOfSet))\n",
        "    counter_vect = Counter()\n",
        "    i = 0\n",
        "    temp_loc =0\n",
        "\n",
        "    for row in X_ngram:\n",
        "        counter_vect = Counter(row)\n",
        "        #print(counter_vect)\n",
        "        for item in row:\n",
        "            if item in vocab:\n",
        "                temp_loc= list_of_vocab.index(item) #get vocab_id of word\n",
        "                X_vec[i][temp_loc] = counter_vect[item]#assign count corresponding to vocabID\n",
        "        i+=1\n",
        "    return X_vec"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:40.219201Z",
          "start_time": "2020-02-15T14:17:40.215129Z"
        },
        "id": "pjOL2ga8Itk5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:27.950323Z",
          "iopub.execute_input": "2025-10-02T12:24:27.950655Z",
          "iopub.status.idle": "2025-10-02T12:24:27.956864Z",
          "shell.execute_reply.started": "2025-10-02T12:24:27.950629Z",
          "shell.execute_reply": "2025-10-02T12:24:27.955855Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:"
      ],
      "metadata": {
        "id": "g5_tMkXTItk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Count vectors"
      ],
      "metadata": {
        "id": "dT4t9meMItk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_count = vectorise(X_ngram,vocab)\n",
        "print(X_tr_count)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:28.145788Z",
          "start_time": "2020-02-15T14:17:28.066100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "ER9lCbWHItk7",
        "outputId": "1ef4433c-d3de-4f57-d767-b4290f9427a7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:31.968177Z",
          "iopub.execute_input": "2025-10-02T12:24:31.968489Z",
          "iopub.status.idle": "2025-10-02T12:24:35.664298Z",
          "shell.execute_reply.started": "2025-10-02T12:24:31.968462Z",
          "shell.execute_reply": "2025-10-02T12:24:35.662966Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_count.shape #(1400, 5000)\n",
        "type(X_tr_count)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:42.004808Z",
          "start_time": "2020-02-15T14:17:42.001555Z"
        },
        "id": "B3QsyjK4Itk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:35.665996Z",
          "iopub.execute_input": "2025-10-02T12:24:35.666371Z",
          "iopub.status.idle": "2025-10-02T12:24:35.675494Z",
          "shell.execute_reply.started": "2025-10-02T12:24:35.666338Z",
          "shell.execute_reply": "2025-10-02T12:24:35.673690Z"
        },
        "outputId": "6f9f8de8-1dea-4bb0-ed7a-65824347d852"
      },
      "outputs": [
        {
          "execution_count": 119,
          "output_type": "execute_result",
          "data": {
            "text/plain": "numpy.ndarray"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_count[:2,:50]"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:42.010525Z",
          "start_time": "2020-02-15T14:17:42.006309Z"
        },
        "id": "22q2MPT6Itk8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:37.522163Z",
          "iopub.execute_input": "2025-10-02T12:24:37.522458Z",
          "iopub.status.idle": "2025-10-02T12:24:37.530561Z",
          "shell.execute_reply.started": "2025-10-02T12:24:37.522437Z",
          "shell.execute_reply": "2025-10-02T12:24:37.529384Z"
        },
        "outputId": "9fdfc9e0-0c08-480d-d051-55aeed40760c"
      },
      "outputs": [
        {
          "execution_count": 121,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Logistic Regression\n",
        "\n",
        "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
      ],
      "metadata": {
        "id": "9ju-zXZfItlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, you need to implement the `sigmoid` function. It takes as input:\n",
        "\n",
        "- `z`: a real number or an array of real numbers\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `sig`: the sigmoid of `z`"
      ],
      "metadata": {
        "id": "O_ttDud4ItlB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
        "\n",
        "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
        "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `preds_proba`: the prediction probabilities of X given the weights"
      ],
      "metadata": {
        "id": "3j1J3y2ZItlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
        "\n",
        "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
        "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `preds_class`: the predicted class for each x in X given the weights"
      ],
      "metadata": {
        "id": "DH3PbLAwItlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
        "\n",
        "- `X`: input vectors\n",
        "- `Y`: labels\n",
        "- `weights`: model weights\n",
        "- `alpha`: regularisation strength\n",
        "\n",
        "and return:\n",
        "\n",
        "- `l`: the loss score"
      ],
      "metadata": {
        "id": "SKxgHtEOItlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
        "\n",
        "- `X_tr`: array of training data (vectors)\n",
        "- `Y_tr`: labels of `X_tr`\n",
        "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
        "- `Y_dev`: labels of `X_dev`\n",
        "- `lr`: learning rate\n",
        "- `alpha`: regularisation strength\n",
        "- `epochs`: number of full passes over the training data\n",
        "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
        "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
        "\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `weights`: the weights learned\n",
        "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
        "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
      ],
      "metadata": {
        "id": "TibY4gOUItlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate Logistic Regression with Count vectors\n",
        "\n",
        "First train the model using SGD:"
      ],
      "metadata": {
        "id": "A9z1AbenItlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Calculate X_dev_count & X_test_count\n",
        "XDev_ngram = Extract_X_ngram(data_dev_textlist)\n",
        "X_dev_count = vectorise(XDev_ngram,vocab)\n",
        "\n",
        "XTest_ngram = Extract_X_ngram(data_test_textlist)\n",
        "X_test_count = vectorise(XTest_ngram,vocab)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:45.673280Z",
          "iopub.execute_input": "2025-10-02T12:24:45.673617Z",
          "iopub.status.idle": "2025-10-02T12:24:47.182539Z",
          "shell.execute_reply.started": "2025-10-02T12:24:45.673574Z",
          "shell.execute_reply": "2025-10-02T12:24:47.181451Z"
        },
        "id": "j6kDnnbU531y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Huáº¥n luyá»‡n Logistic Regression vá»›i sklearn\n",
        "clf = LogisticRegression(\n",
        "    # C=0.05,\n",
        "    # max_iter=1000,          # sá»‘ vÃ²ng láº·p tá»‘i Ä‘a\n",
        "    # solver=\"lbfgs\",         # bá»™ giáº£i (tá»‘i Æ°u hÃ³a) phá»• biáº¿n\n",
        "    # multi_class=\"auto\"      # auto: binary hoáº·c multinomial Ä‘á»u cháº¡y\n",
        ")\n",
        "clf.fit(X_tr_count, ArrayTR.ravel())  # train\n",
        "\n",
        "# Dá»± Ä‘oÃ¡n trÃªn táº­p validation/dev\n",
        "y_pred_dev = clf.predict(X_dev_count)\n",
        "\n",
        "print(\"Classification Report (Dev set):\")\n",
        "print(classification_report(ArrayDev, y_pred_dev))\n",
        "\n",
        "print(\"Confusion Matrix (Dev set):\")\n",
        "print(confusion_matrix(ArrayDev, y_pred_dev))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:48.532998Z",
          "iopub.execute_input": "2025-10-02T12:24:48.534366Z",
          "iopub.status.idle": "2025-10-02T12:24:51.442521Z",
          "shell.execute_reply.started": "2025-10-02T12:24:48.534319Z",
          "shell.execute_reply": "2025-10-02T12:24:51.441671Z"
        },
        "id": "ri_xaktB531y",
        "outputId": "6f9f0307-0a48-41d7-e475-4596d8c20540"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Classification Report (Dev set):\n              precision    recall  f1-score   support\n\n           0       0.72      0.69      0.71       400\n           1       0.70      0.73      0.72       400\n\n    accuracy                           0.71       800\n   macro avg       0.71      0.71      0.71       800\nweighted avg       0.71      0.71      0.71       800\n\nConfusion Matrix (Dev set):\n[[277 123]\n [108 292]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. CÃ¢u má»›i nháº­p\n",
        "new_sentence = [\"best tweet ever!\"]\n",
        "\n",
        "# 2. Chuyá»ƒn cÃ¢u má»›i thÃ nh n-gram giá»‘ng nhÆ° khi train\n",
        "new_sentence_ngram = Extract_X_ngram(new_sentence)\n",
        "\n",
        "# 3. Vectorize báº±ng hÃ m thá»§ cÃ´ng\n",
        "X_new = vectorise(new_sentence_ngram, list_of_vocab)\n",
        "\n",
        "# 4. Dá»± Ä‘oÃ¡n nhÃ£n\n",
        "y_pred_new = clf.predict(X_new)\n",
        "\n",
        "print(\"ðŸ”® Dá»± Ä‘oÃ¡n nhÃ£n cho cÃ¢u:\", new_sentence[0])\n",
        "print(\"ðŸ‘‰ Káº¿t quáº£:\", y_pred_new[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:54.615912Z",
          "iopub.execute_input": "2025-10-02T12:24:54.616778Z",
          "iopub.status.idle": "2025-10-02T12:24:54.625436Z",
          "shell.execute_reply.started": "2025-10-02T12:24:54.616747Z",
          "shell.execute_reply": "2025-10-02T12:24:54.623865Z"
        },
        "id": "qlQKETkb531z",
        "outputId": "23784092-abc0-4242-9bde-7c3c48eea530"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ðŸ”® Dá»± Ä‘oÃ¡n nhÃ£n cho cÃ¢u: best tweet ever!\nðŸ‘‰ Káº¿t quáº£: 1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-class Logistic Regression\n",
        "\n",
        "Now you need to train a Multiclass Logistic Regression (MLR) Classifier by extending the Binary model you developed above. You will use the MLR model to perform topic classification on the AG news dataset consisting of three classes:"
      ],
      "metadata": {
        "id": "O6SJxnABItli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ======================\n",
        "# 1. Chá»n categories\n",
        "# ======================\n",
        "categories = ['sci.med','rec.sport.hockey', 'sci.space', 'talk.religion.misc', 'comp.graphics']\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    remove=('headers','footers','quotes')\n",
        ")\n",
        "newsgroups_test = fetch_20newsgroups(\n",
        "    subset='test',\n",
        "    categories=categories,\n",
        "    remove=('headers','footers','quotes')\n",
        ")\n",
        "\n",
        "# Gá»™p train + test láº¡i Ä‘á»ƒ chá»n subset ~3000\n",
        "all_texts = newsgroups_train.data + newsgroups_test.data\n",
        "all_labels = list(newsgroups_train.target) + list(newsgroups_test.target)\n",
        "\n",
        "# ======================\n",
        "# 2. Láº¥y ngáº«u nhiÃªn 3000 samples\n",
        "# ======================\n",
        "subset_texts, _, subset_labels, _ = train_test_split(\n",
        "    all_texts,\n",
        "    all_labels,\n",
        "    train_size=4000,\n",
        "    stratify=all_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 3. Chia thÃ nh train/dev/test (70/15/15)\n",
        "# ======================\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    subset_texts, subset_labels,\n",
        "    test_size=0.3, random_state=42, stratify=subset_labels\n",
        ")\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 4. Táº¡o DataFrame nhÆ° code gá»‘c\n",
        "# ======================\n",
        "Topic_data_tr = pd.DataFrame({\"label\": y_train, \"text\": X_train})\n",
        "Topic_data_dev = pd.DataFrame({\"label\": y_dev, \"text\": X_dev})\n",
        "Topic_data_test = pd.DataFrame({\"label\": y_test, \"text\": X_test})\n",
        "\n",
        "# ======================\n",
        "# 5. Äá»“ng thá»i táº¡o list + numpy array (format cÅ©)\n",
        "# ======================\n",
        "topic_tr_textlist = Topic_data_tr[[\"text\"]].values.tolist()\n",
        "ArrayTR_Topic = Topic_data_tr[[\"label\"]].to_numpy()\n",
        "\n",
        "topic_dev_textlist = Topic_data_dev[[\"text\"]].values.tolist()\n",
        "ArrayDev_Topic = Topic_data_dev[[\"label\"]].to_numpy()\n",
        "\n",
        "topic_test_textlist = Topic_data_test[[\"text\"]].values.tolist()\n",
        "ArrayTest_Topic = Topic_data_test[[\"label\"]].to_numpy()\n",
        "\n",
        "# ======================\n",
        "# 6. Kiá»ƒm tra\n",
        "# ======================\n",
        "print(\"Train size:\", len(Topic_data_tr))\n",
        "print(\"Dev size:\", len(Topic_data_dev))\n",
        "print(\"Test size:\", len(Topic_data_test))\n",
        "print(\"Sá»‘ lá»›p:\", len(np.unique(ArrayTR_Topic)))\n",
        "\n",
        "Topic_data_tr.head()\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:03.212229Z",
          "start_time": "2020-02-15T14:18:03.185261Z"
        },
        "id": "bNDJ6sX0Itlk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:03.761420Z",
          "iopub.execute_input": "2025-10-02T12:25:03.761759Z",
          "iopub.status.idle": "2025-10-02T12:25:05.993383Z",
          "shell.execute_reply.started": "2025-10-02T12:25:03.761735Z",
          "shell.execute_reply": "2025-10-02T12:25:05.992455Z"
        },
        "outputId": "4a23ff89-b7c5-4709-bf94-7806494b32da"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Train size: 2800\nDev size: 600\nTest size: 600\nSá»‘ lá»›p: 5\n",
          "output_type": "stream"
        },
        {
          "execution_count": 125,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   label                                               text\n0      3  At one time there was speculation that the fir...\n1      0  I am looking for some fast polygon routines (S...\n2      1  Can some on e give me some stats on Forsrg in ...\n3      3  Regarding the feasability of retrieving the HS...\n4      1  -=> Quoting Greg Rogers to All <=-\\n GR> Hi al...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>At one time there was speculation that the fir...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>I am looking for some fast polygon routines (S...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Can some on e give me some stats on Forsrg in ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Regarding the feasability of retrieving the HS...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>-=&gt; Quoting Greg Rogers to All &lt;=-\\n GR&gt; Hi al...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(topic_test_textlist))"
      ],
      "metadata": {
        "id": "rwV__06LItlk",
        "outputId": "f58f1128-4e57-4b7a-e9e3-77429fb0838f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:11.168380Z",
          "iopub.execute_input": "2025-10-02T12:25:11.168732Z",
          "iopub.status.idle": "2025-10-02T12:25:11.174150Z",
          "shell.execute_reply.started": "2025-10-02T12:25:11.168708Z",
          "shell.execute_reply": "2025-10-02T12:25:11.173061Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "600\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Topic_data_tr.head()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:03.515585Z",
          "start_time": "2020-02-15T14:18:03.508299Z"
        },
        "id": "K3PzDaTjItll",
        "outputId": "74447f08-c4fd-42de-cb63-62db4dc342e5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:12.910973Z",
          "iopub.execute_input": "2025-10-02T12:25:12.911302Z",
          "iopub.status.idle": "2025-10-02T12:25:12.919993Z",
          "shell.execute_reply.started": "2025-10-02T12:25:12.911277Z",
          "shell.execute_reply": "2025-10-02T12:25:12.919096Z"
        }
      },
      "outputs": [
        {
          "execution_count": 127,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   label                                               text\n0      3  At one time there was speculation that the fir...\n1      0  I am looking for some fast polygon routines (S...\n2      1  Can some on e give me some stats on Forsrg in ...\n3      3  Regarding the feasability of retrieving the HS...\n4      1  -=> Quoting Greg Rogers to All <=-\\n GR> Hi al...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>At one time there was speculation that the fir...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>I am looking for some fast polygon routines (S...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Can some on e give me some stats on Forsrg in ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Regarding the feasability of retrieving the HS...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>-=&gt; Quoting Greg Rogers to All &lt;=-\\n GR&gt; Hi al...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "topic_vocab, topic_df, topic_ngram_counts_tr = get_vocab(topic_tr_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
        "#topic_vocab, topic_df_dev, topic_ngram_counts_dev = get_vocab(topic_dev_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
        "#topic_vocab,topic_df_test,topic_ngram_counts_test = get_vocab(topic_test_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:03.806523Z",
          "start_time": "2020-02-15T14:18:03.798279Z"
        },
        "id": "PJxxIxHMItlm",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:14.963327Z",
          "iopub.execute_input": "2025-10-02T12:25:14.963670Z",
          "iopub.status.idle": "2025-10-02T12:25:37.740708Z",
          "shell.execute_reply.started": "2025-10-02T12:25:14.963644Z",
          "shell.execute_reply": "2025-10-02T12:25:37.739678Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_vocab = list(topic_vocab)\n",
        "vocab_id_topic = {i:list_of_vocab[i] for i in range(len(topic_vocab))}"
      ],
      "metadata": {
        "id": "xks459SjItln",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:43.386972Z",
          "iopub.execute_input": "2025-10-02T12:25:43.387297Z",
          "iopub.status.idle": "2025-10-02T12:25:43.393812Z",
          "shell.execute_reply.started": "2025-10-02T12:25:43.387271Z",
          "shell.execute_reply": "2025-10-02T12:25:43.392673Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(topic_vocab))\n",
        "print(list(topic_vocab)[:100])\n",
        "print()\n",
        "print(topic_df.most_common()[:10])\n",
        "#print(list_of_vocab[:10])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:04.508938Z",
          "start_time": "2020-02-15T14:18:04.171071Z"
        },
        "scrolled": true,
        "id": "v-x8qDnWItlw",
        "outputId": "f043e37f-5cc5-4c38-c51c-181ff4facdb5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:46.139875Z",
          "iopub.execute_input": "2025-10-02T12:25:46.140226Z",
          "iopub.status.idle": "2025-10-02T12:25:46.256760Z",
          "shell.execute_reply.started": "2025-10-02T12:25:46.140200Z",
          "shell.execute_reply": "2025-10-02T12:25:46.255685Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "5000\n['quiet', 'diagnosis', 'HELP', 'unc', 'newly', 'even though', 'Suite', 'He', 'nGeorge', 'circle', 'bother', 'meet', 've heard', 'unusual', 'OF', 'nChris', 'okay', 'nI agree', 'craft', 'nDetroit', 'Analysis', 'controlled', 'Centre', 'NYR', 'Young', 'studied', 'massive', 'shameful surrender too', 'resolution', 'combination', 'capabilities', 'npoint', 'wing', 'countries', 'requests', 'difference between', 'library', 'Bay', 'justify', 'hours', 'forever', 'one another', 'involves', 'format', 'engineers', 'single', 'nThanks', 'ON', 'non', 'centers', 'nBy', 'Canadiens', 'acts', 'schools', 'nalso', 'ESPN', 'landing', 'Central', 'last night', 'cult', 'occurred', 'all sorts', 'might', 'farm', 'York', 'magazine', 'effective', 'classic', 'guidelines', 'launched', 'edu au', 'wondering', 'funding', 'bad', 'function', 'delivery', 'America', 'impact', 'powerful', 'heat', 'prevention', 'Hope', 'arbitrary', 'offering', 'heart', 'cl msu', 'nmaybe', 'voice', 'attached', 'heavily', 'would say', 'versus', 'via mail', 'uk', 'bearing', 'Mission', 'dates', 'nDon', 'message', 'but ve']\n\n[('not', 1029), ('but', 960), ('would', 776), ('if', 764), ('about', 744), ('nI', 730), ('one', 649), ('all', 624), ('some', 614), ('out', 613)]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "##Counts\n",
        "\n",
        "topic_X_tr_ngram = Extract_X_ngram(topic_tr_textlist)\n",
        "topic_X_tr_count = vectorise(topic_X_tr_ngram,topic_vocab)\n",
        "\n",
        "topic_XDev_ngram = Extract_X_ngram(topic_dev_textlist)\n",
        "topic_X_dev_count = vectorise(topic_XDev_ngram,topic_vocab)\n",
        "\n",
        "topic_XTest_ngram = Extract_X_ngram(topic_test_textlist)\n",
        "topic_X_test_count = vectorise(topic_XTest_ngram,topic_vocab)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:04.706802Z",
          "start_time": "2020-02-15T14:18:04.511061Z"
        },
        "id": "RnBQSkOaItlx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:48.693181Z",
          "iopub.execute_input": "2025-10-02T12:25:48.693483Z",
          "iopub.status.idle": "2025-10-02T12:26:34.492833Z",
          "shell.execute_reply.started": "2025-10-02T12:25:48.693462Z",
          "shell.execute_reply": "2025-10-02T12:26:34.491698Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ======================\n",
        "# 1. Train vá»›i Count Vectors\n",
        "# ======================\n",
        "sgd_count = SGDClassifier(\n",
        "    loss=\"log_loss\",      # logistic regression\n",
        "    penalty=\"l2\",         # regularization\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "sgd_count.fit(topic_X_tr_count, ArrayTR_Topic.ravel())\n",
        "\n",
        "# Evaluate trÃªn Dev set\n",
        "y_dev_pred_count = sgd_count.predict(topic_X_dev_count)\n",
        "print(\"=== Logistic Regression (Count Vectors) ===\")\n",
        "print(classification_report(ArrayDev_Topic, y_dev_pred_count))\n",
        "print(confusion_matrix(ArrayDev_Topic, y_dev_pred_count))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:28:07.593311Z",
          "iopub.execute_input": "2025-10-02T12:28:07.593669Z",
          "iopub.status.idle": "2025-10-02T12:28:10.685209Z",
          "shell.execute_reply.started": "2025-10-02T12:28:07.593643Z",
          "shell.execute_reply": "2025-10-02T12:28:10.683958Z"
        },
        "id": "B9RRUwxa5312",
        "outputId": "d94fac96-1c30-48e6-fe74-f1f1dc7f4f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "=== Logistic Regression (Count Vectors) ===\n              precision    recall  f1-score   support\n\n           0       0.84      0.87      0.85       127\n           1       0.88      0.85      0.87       131\n           2       0.78      0.85      0.81       130\n           3       0.84      0.75      0.79       129\n           4       0.74      0.76      0.75        83\n\n    accuracy                           0.82       600\n   macro avg       0.82      0.82      0.81       600\nweighted avg       0.82      0.82      0.82       600\n\n[[110   3  10   2   2]\n [  1 112   6   5   7]\n [  9   0 110   6   5]\n [  9   4  11  97   8]\n [  2   8   4   6  63]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Input má»›i\n",
        "new_sentence = [\"Astronomers discovered a new exoplanet orbiting a distant star, raising hopes for extraterrestrial life\"]\n",
        "\n",
        "# 2. N-gram hÃ³a (giá»‘ng lÃºc train)\n",
        "new_sentence_ngram = Extract_X_ngram(new_sentence)\n",
        "\n",
        "# 3. Vectorize vá»›i vocab cÅ©\n",
        "X_new_count = vectorise(new_sentence_ngram, list_of_vocab)  # Count vector\n",
        "\n",
        "# 4. Predict vá»›i mÃ´ hÃ¬nh Count\n",
        "y_pred_new_count = sgd_count.predict(X_new_count)\n",
        "\n",
        "print(\"ðŸ”® CÃ¢u:\", new_sentence[0])\n",
        "print(\"ðŸ‘‰ Dá»± Ä‘oÃ¡n (Count):\", y_pred_new_count[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:28:17.231421Z",
          "iopub.execute_input": "2025-10-02T12:28:17.231767Z",
          "iopub.status.idle": "2025-10-02T12:28:17.242361Z",
          "shell.execute_reply.started": "2025-10-02T12:28:17.231743Z",
          "shell.execute_reply": "2025-10-02T12:28:17.241075Z"
        },
        "id": "FlNmVnKx5313",
        "outputId": "584f72ba-6b76-468d-df5d-554bd5380030"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ðŸ”® CÃ¢u: Astronomers discovered a new exoplanet orbiting a distant star, raising hopes for extraterrestrial life\nðŸ‘‰ Dá»± Ä‘oÃ¡n (Count): 3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you need to compute the categorical cross entropy loss (extending the binary loss to support multiple classes)."
      ],
      "metadata": {
        "id": "4DQAwChkItl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Láº¥y láº¡i categories báº¡n dÃ¹ng khi load dataset\n",
        "categories = ['sci.med','rec.sport.hockey', 'sci.space', 'talk.religion.misc', 'comp.graphics']\n",
        "\n",
        "# Load dataset train\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    remove=('headers','footers','quotes')\n",
        ")\n",
        "\n",
        "# In cÃ¡c class sá»‘\n",
        "print(\"CÃ¡c class sá»‘:\", np.unique(newsgroups_train.target))\n",
        "\n",
        "# In mapping class sá»‘ -> label\n",
        "for i, name in enumerate(newsgroups_train.target_names):\n",
        "    print(f\"{i} â†’ {name}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:28:19.969072Z",
          "iopub.execute_input": "2025-10-02T12:28:19.969372Z",
          "iopub.status.idle": "2025-10-02T12:28:21.299407Z",
          "shell.execute_reply.started": "2025-10-02T12:28:19.969351Z",
          "shell.execute_reply": "2025-10-02T12:28:21.298540Z"
        },
        "id": "CF4NjYMF5313",
        "outputId": "291423d1-620a-4ba8-e772-380c706ccd27"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "CÃ¡c class sá»‘: [0 1 2 3 4]\n0 â†’ comp.graphics\n1 â†’ rec.sport.hockey\n2 â†’ sci.med\n3 â†’ sci.space\n4 â†’ talk.religion.misc\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph outputted from HPC![NLP1.png](attachment:NLP1.png)"
      ],
      "metadata": {
        "id": "hbThhI9lItl6"
      }
    }
  ]
}
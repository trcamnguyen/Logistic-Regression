{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 4140,
          "sourceType": "datasetVersion",
          "datasetId": 2477
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trcamnguyen/Logistic-Regression/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/repo-name/blob/main/notebook.ipynb)\n"
      ],
      "metadata": {
        "id": "qcwrCbfQ9y3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression for Text Classification\n",
        "\n",
        "## Overview\n",
        "This notebook implements and evaluates **Logistic Regression models** for text classification tasks:\n",
        "- **Binary Logistic Regression**: Sentiment analysis (positive vs negative)\n",
        "- **Multinomial Logistic Regression**: Multi-class text classification (softmax regression)\n",
        "\n",
        "## Objectives\n",
        "- Build Logistic Regression models for both binary and multi-class text data\n",
        "- Understand differences between binary vs multinomial logistic regression\n",
        "- Evaluate model quality with accuracy, precision, recall, F1-score, and confusion matrix\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Outcomes\n",
        "- **Binary Logistic Regression**: Effective sentiment classifier with clear positive/negative separation  \n",
        "- **Multinomial Logistic Regression**: Generalized model for multi-class problems using softmax  \n",
        "- **Evaluation Metrics**: Demonstrated on both binary and multi-class datasets  \n",
        "- **Insights**: Comparison of logistic regression behavior in binary vs multinomial settings  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0-1GmK0gD8sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from scipy.special import softmax as sftmx #used to test my function, allowed\n",
        "# fixing random seed for reproducibility\n",
        "random.seed(123)\n",
        "np.random.seed(123)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:31:36.292691Z",
          "start_time": "2020-02-15T14:31:35.549108Z"
        },
        "id": "tfe_F1IAItkk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:23:58.691471Z",
          "iopub.execute_input": "2025-10-02T12:23:58.692180Z",
          "iopub.status.idle": "2025-10-02T12:23:58.698008Z",
          "shell.execute_reply.started": "2025-10-02T12:23:58.692151Z",
          "shell.execute_reply": "2025-10-02T12:23:58.696870Z"
        }
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I. Binary Logistic Regression (Sentiment Analysis)\n",
        "\n",
        "### Step 1. **Data Loading**\n",
        "- **Dataset**: Import sentiment dataset (e.g., Sentiment140 or custom text dataset)\n",
        "- **File Access**: Load text samples and corresponding binary sentiment labels\n",
        "- **Initial Inspection**: Preview data distribution (positive vs negative)\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2. **Data Preprocessing**\n",
        "- **Text Cleaning**: Remove punctuation, digits, and special characters  \n",
        "- **Normalization**: Convert to lowercase  \n",
        "- **Tokenization**: Split text into words or tokens  \n",
        "- **Stopword Removal** *(optional)*  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 3. **Feature Representation**\n",
        "- **Bag-of-Words**: Convert tokenized text into numerical feature vectors  \n",
        "- **Vocabulary Construction**: Build word-index mapping  \n",
        "- **Binary/Count Representation**: Represent presence or frequency of terms  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 4. **Model Construction**\n",
        "- **Hypothesis Function**:  \n",
        "  $$ h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}} $$\n",
        "- **Loss Function (Binary Cross-Entropy)**:  \n",
        "  $$ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\Big[y^{(i)}\\log h_\\theta(x^{(i)}) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\Big] $$\n",
        "- **Training**: Fit logistic regression classifier on binary labels  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 5. **Evaluation**\n",
        "- **Metrics**: Accuracy, Precision, Recall, F1-score  \n",
        "- **Confusion Matrix**: Visualize predictions vs ground truth  \n",
        "- **Error Analysis**: Inspect misclassified examples"
      ],
      "metadata": {
        "id": "OdgfwX6pItkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "# Download Sentiment140\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "file_path = path + \"/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "cols = ['target','id','date','flag','user','text']\n",
        "df = pd.read_csv(file_path, encoding='ISO-8859-1', names=cols)\n",
        "\n",
        "# Giữ lại text + label, đổi 4 -> 1\n",
        "df = df[['text','target']]\n",
        "df['target'] = df['target'].replace(4,1)\n",
        "\n",
        "# Lấy subset nhỏ gọn 10,000 dòng\n",
        "df_small = df.sample(n=10000, random_state=42)\n",
        "\n",
        "# Chia train/dev/test\n",
        "train_df, test_df = train_test_split(df_small, test_size=0.2, random_state=42, stratify=df_small['target'])\n",
        "train_df, dev_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['target'])\n",
        "\n",
        "# Format\n",
        "data_tr_textlist = train_df[['text']].values.tolist()\n",
        "ArrayTR = train_df[['target']].to_numpy()\n",
        "\n",
        "data_dev_textlist = dev_df[['text']].values.tolist()\n",
        "ArrayDev = dev_df[['target']].to_numpy()\n",
        "\n",
        "data_test_textlist = test_df[['text']].values.tolist()\n",
        "ArrayTest = test_df[['target']].to_numpy()\n",
        "\n",
        "print(\"Binary dataset size:\", len(df_small))\n",
        "print(\"Train/Dev/Test:\", len(data_tr_textlist), len(data_dev_textlist), len(data_test_textlist))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyd1BBp1Itkp",
        "outputId": "f268f6fc-d597-4a45-91a2-8ad3133b8af9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:02.361903Z",
          "iopub.execute_input": "2025-10-02T12:24:02.362768Z",
          "iopub.status.idle": "2025-10-02T12:24:07.176997Z",
          "shell.execute_reply.started": "2025-10-02T12:24:02.362738Z",
          "shell.execute_reply": "2025-10-02T12:24:07.176011Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'sentiment140' dataset.\n",
            "Binary dataset size: 10000\n",
            "Train/Dev/Test: 7200 800 2000\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = ['a','in','on','at','and','or',\n",
        "              'to', 'the', 'of', 'an', 'by',\n",
        "              'as', 'is', 'was', 'were', 'been', 'be',\n",
        "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
        "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
        "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
        "             'his', 'her', 'they', 'them', 'from', 'with', 'its','also','so','there','their','The']"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:31.860420Z",
          "start_time": "2020-02-15T14:17:31.855439Z"
        },
        "id": "HRspqqpRItkt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:09.227122Z",
          "iopub.execute_input": "2025-10-02T12:24:09.227443Z",
          "iopub.status.idle": "2025-10-02T12:24:09.234062Z",
          "shell.execute_reply.started": "2025-10-02T12:24:09.227418Z",
          "shell.execute_reply": "2025-10-02T12:24:09.232544Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gram extraction from a document\n",
        "\n",
        "You first need to implement the `extract_ngrams` function. It takes as input:\n",
        "- `x_raw`: a string corresponding to the raw text of a document\n",
        "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
        "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
        "- `stop_words`: a list of stop words\n",
        "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
        "\n",
        "and returns:\n",
        "\n",
        "- a list of all extracted features.\n",
        "\n",
        "See the examples below to see how this function should work."
      ],
      "metadata": {
        "id": "lUOnMJC_Itkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', stop_words= stop_words, vocab=set()):\n",
        "\n",
        "\n",
        "    ####Tokenisation#######\n",
        "    com = re.compile(token_pattern)\n",
        "    x_raw = com.findall(x_raw)\n",
        "    # vocab = re.findall(token_pattern,vocab)\n",
        "\n",
        "   # x_raw = list(x_raw.split())     #Split Sentence into seperate words then store as list\n",
        "\n",
        "\n",
        " #####Remove Stop words########\n",
        "    x_raw=[word for word in x_raw if word not in stop_words]\n",
        "    for word in stop_words:\n",
        "        for word2 in x_raw:\n",
        "            if word == word2:\n",
        "                x_raw.remove(word)\n",
        "\n",
        "\n",
        "\n",
        "    #print(vocab)\n",
        "\n",
        "#######Return Vocab#####\n",
        "    #vocab needs to normalised\n",
        "    vocab1 = str(vocab)\n",
        "    vocab1 = re.findall(token_pattern,vocab1)\n",
        "    commachar = \",\"\n",
        "    spacechar = ''\n",
        "    if ((len(vocab1)) != 0): #Check if set is empty\n",
        "        vocablist = []\n",
        "        for word in x_raw:\n",
        "            for word2 in vocab1:\n",
        "                if ((word == word2) & (word != commachar) & (word != spacechar)& (word2 not in vocablist)): #remove commas & match words in text &vocab\n",
        "                    vocablist.append(word.replace(\" \", \"\")) ## remove whitespace characters\n",
        "                   # print(vocablist)\n",
        "\n",
        "    noofngrams=[]\n",
        "    ngrams_list = []\n",
        "    if ngram_range == (1,3):\n",
        "        noofngrams= [1,2,3]\n",
        "    if ngram_range == (1,2):\n",
        "        noofngrams= [1,2]\n",
        "\n",
        "    y =[]\n",
        "\n",
        "    #Extract Ngrams\n",
        "    for n in noofngrams:\n",
        "        if (len(vocab)) == 0:\n",
        "                for num in range(0, len(x_raw)):\n",
        "                    ngram = ' '.join(x_raw[num:num + n])\n",
        "                    #if ngram not in ngrams_list:\n",
        "                    if ((ngram != '')):\n",
        "                        ngrams_list.append(ngram)\n",
        "                        if ngram not in y:\n",
        "                            y.append(ngram)\n",
        "\n",
        "        if (len(vocab)) != 0:\n",
        "                for n in range(0,len(vocab1)):\n",
        "                    for num in range(0, len(vocablist)):\n",
        "                        if len(vocablist[num]) != 0:\n",
        "                                ngram = ' '.join(vocablist[num:num + n])\n",
        "\n",
        "                                if ((ngram != '')): # ((ngram not in ngrams_list) &\n",
        "                                    ngrams_list.append(ngram)\n",
        "\n",
        "                                    if ngram not in y:\n",
        "                                        y.append(ngram)\n",
        "\n",
        "    x= ngrams_list\n",
        "    return x,y # y is unique ngram list, x is list including duplicates for vectorisation"
      ],
      "metadata": {
        "id": "CrI3-VziItku",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:11.865374Z",
          "iopub.execute_input": "2025-10-02T12:24:11.865715Z",
          "iopub.status.idle": "2025-10-02T12:24:11.877557Z",
          "shell.execute_reply.started": "2025-10-02T12:24:11.865692Z",
          "shell.execute_reply": "2025-10-02T12:24:11.876617Z"
        }
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "x1,y1 = extract_ngrams(\"this is a great movie to watch\",\n",
        "               ngram_range=(1,3),\n",
        "               stop_words=stop_words)\n",
        "print(y1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:33.680114Z",
          "start_time": "2020-02-15T14:17:33.675339Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmbRpQasItkw",
        "outputId": "6def676c-872f-4a52-b430-d1851c50915a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:14.774636Z",
          "iopub.execute_input": "2025-10-02T12:24:14.774969Z",
          "iopub.status.idle": "2025-10-02T12:24:14.780949Z",
          "shell.execute_reply.started": "2025-10-02T12:24:14.774949Z",
          "shell.execute_reply": "2025-10-02T12:24:14.779919Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['great', 'movie', 'watch', 'great movie', 'movie watch', 'great movie watch']\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
      ],
      "metadata": {
        "id": "jWYnA9xOItky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a vocabulary of n-grams\n",
        "\n",
        "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
        "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
        "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
        "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
        "- `stop_words`: a list of stop words\n",
        "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
        "- `min_df`: keep ngrams with a minimum document frequency.\n",
        "- `keep_topN`: keep top-N more frequent ngrams.\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `vocab`: a set of the n-grams that will be used as features.\n",
        "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
        "- `ngram_counts`: counts of each ngram in vocab\n",
        "\n",
        "Hint: it should make use of the `extract_ngrams` function."
      ],
      "metadata": {
        "id": "8un6kwCMItkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=0, stop_words=[]):\n",
        "\n",
        "    #Define empty placeholder variables\n",
        "    df=  Counter()\n",
        "    ngram_counts = Counter()\n",
        "    vocab = set()\n",
        "    df1 = list()\n",
        "    df2 = Counter()\n",
        "    #com = re.compile(token_pattern)\n",
        "    #X_raw = com.findall(X_raw)\n",
        "    vocab1 = set()\n",
        "\n",
        "    for x_raw in X_raw:\n",
        "        x_raw = str(x_raw)\n",
        "        com = re.compile(token_pattern)\n",
        "        x_raw = com.findall(x_raw)\n",
        "        x_raw = str(x_raw)\n",
        "        x_raw.replace(\" \", \"\")\n",
        "        x_raw.replace(\"[\",\"\")\n",
        "        x_raw.replace(\"]\",\"\")\n",
        "        y,x = extract_ngrams(x_raw, ngram_range, token_pattern, stop_words)\n",
        "        #calling extract_ngrams deals with tokenisation,stop words,ngram range and ngram extraction\n",
        "        for item in x:\n",
        "            df[item] += 1\n",
        "            if df[item]> min_df: # Only keep above the minimum\n",
        "                df2[item] += 1\n",
        "\n",
        "            if (str(item) not in vocab) & (len(vocab) < keep_topN): #stop adding to vocab once cap is reached\n",
        "            #vocab.add(str(x))\n",
        "                 vocab.add(str(item)) #need to remove brackets as i think its passing\n",
        "\n",
        "        for string in x:\n",
        "            if string in ngram_counts:\n",
        "                    ngram_counts[string] += 1 #len 998359\n",
        "            else:\n",
        "                    ngram_counts[string] = 1\n",
        "    #df1.append(df2.most_common(keep_topN)) #Keep only the specified amount of N\n",
        "    #ngram_counts.append(ngram_counts.most_common(keep_topN))\n",
        "    #vocab = list(vocab)\n",
        "    df = df2\n",
        "\n",
        "\n",
        "#######assign top ngrams to vocab set###########################\n",
        "   # vocab1 = set()\n",
        "    for item in ngram_counts.most_common(keep_topN):\n",
        "\n",
        "            # type(item)) item is a tupple\n",
        "        if (len(vocab1) < keep_topN)  :\n",
        "                vocab1.add(item[0])\n",
        "\n",
        "    vocab = vocab1\n",
        "    return vocab, df, ngram_counts"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:35.821240Z",
          "start_time": "2020-02-15T14:17:35.814722Z"
        },
        "id": "nfhxA8jEItk0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:17.325291Z",
          "iopub.execute_input": "2025-10-02T12:24:17.325640Z",
          "iopub.status.idle": "2025-10-02T12:24:17.334305Z",
          "shell.execute_reply.started": "2025-10-02T12:24:17.325606Z",
          "shell.execute_reply": "2025-10-02T12:24:17.333276Z"
        }
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
      ],
      "metadata": {
        "id": "dYwdXNWZItk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, df, ngram_counts = get_vocab(data_tr_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
        "print(len(vocab))\n",
        "print()\n",
        "print(list(vocab)[:100])\n",
        "print()\n",
        "print(df.most_common()[:10])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:39.319793Z",
          "start_time": "2020-02-15T14:17:36.836545Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BCE_lc-Itk1",
        "outputId": "a64876fe-6ab3-4f37-bac6-ce648ba6c88d",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:20.217581Z",
          "iopub.execute_input": "2025-10-02T12:24:20.217925Z",
          "iopub.status.idle": "2025-10-02T12:24:21.221749Z",
          "shell.execute_reply.started": "2025-10-02T12:24:20.217902Z",
          "shell.execute_reply": "2025-10-02T12:24:21.220789Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "\n",
            "['conversation', 'gotta go', 'hadn', 'recovering', 'Sun', 'had dog', 'using', 'if like', 'learn how', 'visit', 'theme', 'Help', 'turned', 'admit', 'Cant wait', 'But no', 'my office', 'kinda sad', 'whole thing', 'posted', 'la', 'challenge', 'reason', 'every', 'just go', 'Busy', 'one me', 'survived', 'trust', 'battery', 'margaritas', 'Maybe next', 'feels', 'bringing', 'Much', 'tired my', 'Wow', 'flying', 'Liverpool', 'cousin', 'yet', 'platform', 'things', 'skin', 'daughter', 'homee', 'parents', 'Went', 'day using www', 'Men', 'asking', 'least', 'if re', 'Dinner', 'studies', 'tears', 'feet hurt', 'much', 'studio', 'vote', 'work now', 'cares', 'but still', 'not good', 'some reason', 'awhile', 'me some', 'good night sleep', 'invite', 'gift', 'drama', 'Cant', 'during', 'server', 'wanna', 'easily', 've always', 'nonsense', 'out here', 'MORE', 'all my followers', 'Coffee', 'Yeah', 'wanting', 'unfair', 'com my', 'see play', 'sound', 'burnt', 'smile my face', 'cd', 'me please', 'Aww man', 'himself', 'me right', 'nail', 'XD', 'smells', 'Brazil', 'answer me']\n",
            "\n",
            "[('my', 1124), ('me', 640), ('but', 499), ('not', 419), ('just', 416), ('up', 378), ('get', 340), ('good', 339), ('now', 332), ('all', 332)]\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, you need to create vocabulary id -> word and id -> word dictionaries for reference:"
      ],
      "metadata": {
        "id": "aDUK6EtxItk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_vocab = list(vocab)\n",
        "vocab_id = {i:list_of_vocab[i] for i in range(len(vocab))}"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:39.326811Z",
          "start_time": "2020-02-15T14:17:39.322256Z"
        },
        "id": "-eU1uRMgItk2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:23.668870Z",
          "iopub.execute_input": "2025-10-02T12:24:23.669167Z",
          "iopub.status.idle": "2025-10-02T12:24:23.676168Z",
          "shell.execute_reply.started": "2025-10-02T12:24:23.669145Z",
          "shell.execute_reply": "2025-10-02T12:24:23.675225Z"
        }
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you should be able to extract n-grams for each text in the training, development and test sets:"
      ],
      "metadata": {
        "id": "i6LEY9FnItk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorise documents"
      ],
      "metadata": {
        "id": "islhfNHEItk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
        "- `X_ngram`: a list of texts (documents), where each text (doc) is represented as list of n-grams in the `vocab`\n",
        "- `vocab`: a set of n-grams to be used for representing the documents\n",
        "\n",
        "and return:\n",
        "- `X_vec`: an array with dimensionality N x |vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
      ],
      "metadata": {
        "id": "Mw8BuNP2Itk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Take training set as input\n",
        "#divide into doc\n",
        "#extract each ngram for each doc\n",
        "#Count ngram in doc using counter\n",
        "#iterate counter through the vocab and assign counts in that order\n",
        "def Extract_X_ngram(data_tr_textlist):\n",
        "    X_ngram =[]\n",
        "    for line in data_tr_textlist:\n",
        "        ngram,p = list(extract_ngrams(str(line),stop_words= stop_words))\n",
        "        X_ngram.append(ngram)\n",
        "    return X_ngram\n",
        "\n",
        "X_ngram = Extract_X_ngram(data_tr_textlist)\n",
        "#print(Counter(X_ngram[0]))"
      ],
      "metadata": {
        "id": "c9qF2QbIItk4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:25.840387Z",
          "iopub.execute_input": "2025-10-02T12:24:25.840766Z",
          "iopub.status.idle": "2025-10-02T12:24:26.191870Z",
          "shell.execute_reply.started": "2025-10-02T12:24:25.840728Z",
          "shell.execute_reply": "2025-10-02T12:24:26.190963Z"
        }
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorise(X_ngram, vocab):\n",
        "    N = len(X_ngram) #Get Dimensions #1400 x 5000 for training set\n",
        "    SizeOfSet = len(vocab)\n",
        "    X_vec = np.zeros((N,SizeOfSet))\n",
        "    counter_vect = Counter()\n",
        "    i = 0\n",
        "    temp_loc =0\n",
        "\n",
        "    for row in X_ngram:\n",
        "        counter_vect = Counter(row)\n",
        "        #print(counter_vect)\n",
        "        for item in row:\n",
        "            if item in vocab:\n",
        "                temp_loc= list_of_vocab.index(item) #get vocab_id of word\n",
        "                X_vec[i][temp_loc] = counter_vect[item]#assign count corresponding to vocabID\n",
        "        i+=1\n",
        "    return X_vec"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:40.219201Z",
          "start_time": "2020-02-15T14:17:40.215129Z"
        },
        "id": "pjOL2ga8Itk5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:27.950323Z",
          "iopub.execute_input": "2025-10-02T12:24:27.950655Z",
          "iopub.status.idle": "2025-10-02T12:24:27.956864Z",
          "shell.execute_reply.started": "2025-10-02T12:24:27.950629Z",
          "shell.execute_reply": "2025-10-02T12:24:27.955855Z"
        }
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Count vectors"
      ],
      "metadata": {
        "id": "dT4t9meMItk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_count = vectorise(X_ngram,vocab)\n",
        "print(X_tr_count)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:28.145788Z",
          "start_time": "2020-02-15T14:17:28.066100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER9lCbWHItk7",
        "outputId": "32d07dd0-1ae9-4f2f-bb20-7320cbb1e853",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:31.968177Z",
          "iopub.execute_input": "2025-10-02T12:24:31.968489Z",
          "iopub.status.idle": "2025-10-02T12:24:35.664298Z",
          "shell.execute_reply.started": "2025-10-02T12:24:31.968462Z",
          "shell.execute_reply": "2025-10-02T12:24:35.662966Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_count.shape #(1400, 5000)\n",
        "type(X_tr_count)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:42.004808Z",
          "start_time": "2020-02-15T14:17:42.001555Z"
        },
        "id": "B3QsyjK4Itk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:35.665996Z",
          "iopub.execute_input": "2025-10-02T12:24:35.666371Z",
          "iopub.status.idle": "2025-10-02T12:24:35.675494Z",
          "shell.execute_reply.started": "2025-10-02T12:24:35.666338Z",
          "shell.execute_reply": "2025-10-02T12:24:35.673690Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b231e846-b432-4f9d-92c0-81e6b91791cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_count[:2,:50]"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:42.010525Z",
          "start_time": "2020-02-15T14:17:42.006309Z"
        },
        "id": "22q2MPT6Itk8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:37.522163Z",
          "iopub.execute_input": "2025-10-02T12:24:37.522458Z",
          "iopub.status.idle": "2025-10-02T12:24:37.530561Z",
          "shell.execute_reply.started": "2025-10-02T12:24:37.522437Z",
          "shell.execute_reply": "2025-10-02T12:24:37.529384Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59fd3d3b-6e00-47c8-9efc-c1a8ec58b43c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate Logistic Regression with Count vectors\n"
      ],
      "metadata": {
        "id": "A9z1AbenItlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Calculate X_dev_count & X_test_count\n",
        "XDev_ngram = Extract_X_ngram(data_dev_textlist)\n",
        "X_dev_count = vectorise(XDev_ngram,vocab)\n",
        "\n",
        "XTest_ngram = Extract_X_ngram(data_test_textlist)\n",
        "X_test_count = vectorise(XTest_ngram,vocab)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:45.673280Z",
          "iopub.execute_input": "2025-10-02T12:24:45.673617Z",
          "iopub.status.idle": "2025-10-02T12:24:47.182539Z",
          "shell.execute_reply.started": "2025-10-02T12:24:45.673574Z",
          "shell.execute_reply": "2025-10-02T12:24:47.181451Z"
        },
        "id": "j6kDnnbU531y"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "# Huấn luyện Logistic Regression với sklearn\n",
        "clf = LogisticRegression(\n",
        "    # C=0.05,\n",
        "    # max_iter=1000,          # số vòng lặp tối đa\n",
        "    # solver=\"lbfgs\",         # bộ giải (tối ưu hóa) phổ biến\n",
        "    # multi_class=\"auto\"      # auto: binary hoặc multinomial đều chạy\n",
        ")\n",
        "clf.fit(X_tr_count, ArrayTR.ravel())  # train\n",
        "\n",
        "# Dự đoán trên tập validation/dev\n",
        "y_pred_dev = clf.predict(X_dev_count)\n",
        "\n",
        "print(\"Classification Report (Dev set):\")\n",
        "print(classification_report(ArrayDev, y_pred_dev))\n",
        "\n",
        "print(\"Confusion Matrix (Dev set):\")\n",
        "print(confusion_matrix(ArrayDev, y_pred_dev))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:48.532998Z",
          "iopub.execute_input": "2025-10-02T12:24:48.534366Z",
          "iopub.status.idle": "2025-10-02T12:24:51.442521Z",
          "shell.execute_reply.started": "2025-10-02T12:24:48.534319Z",
          "shell.execute_reply": "2025-10-02T12:24:51.441671Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri_xaktB531y",
        "outputId": "60aa2982-40ca-41ef-e359-ab87e72df239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Dev set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.69      0.71       400\n",
            "           1       0.71      0.73      0.72       400\n",
            "\n",
            "    accuracy                           0.71       800\n",
            "   macro avg       0.71      0.71      0.71       800\n",
            "weighted avg       0.71      0.71      0.71       800\n",
            "\n",
            "Confusion Matrix (Dev set):\n",
            "[[278 122]\n",
            " [108 292]]\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = clf.predict(X_test_count)\n",
        "\n",
        "print(\"\\n=== Evaluation on Test set ===\")\n",
        "print(\"Classification Report (Test set):\")\n",
        "print(classification_report(ArrayTest, y_pred_test))\n",
        "print(\"Confusion Matrix (Test set):\")\n",
        "print(confusion_matrix(ArrayTest, y_pred_test))\n",
        "print(\"Accuracy:\", accuracy_score(ArrayTest, y_pred_test))\n",
        "print(\"Precision:\", precision_score(ArrayTest, y_pred_test))\n",
        "print(\"Recall:\", recall_score(ArrayTest, y_pred_test))\n",
        "print(\"F1-Score:\", f1_score(ArrayTest, y_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ClQ7dMFL9UI",
        "outputId": "d0b7324b-50be-40e5-ba11-684561326d1e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluation on Test set ===\n",
            "Classification Report (Test set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.70      0.71      1001\n",
            "           1       0.71      0.72      0.71       999\n",
            "\n",
            "    accuracy                           0.71      2000\n",
            "   macro avg       0.71      0.71      0.71      2000\n",
            "weighted avg       0.71      0.71      0.71      2000\n",
            "\n",
            "Confusion Matrix (Test set):\n",
            "[[703 298]\n",
            " [279 720]]\n",
            "Accuracy: 0.7115\n",
            "Precision: 0.7072691552062869\n",
            "Recall: 0.7207207207207207\n",
            "F1-Score: 0.7139315815567675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Câu mới nhập\n",
        "new_sentence = [\"best tweet ever!\"]\n",
        "\n",
        "# 2. Chuyển câu mới thành n-gram giống như khi train\n",
        "new_sentence_ngram = Extract_X_ngram(new_sentence)\n",
        "\n",
        "# 3. Vectorize bằng hàm thủ công\n",
        "X_new = vectorise(new_sentence_ngram, list_of_vocab)\n",
        "\n",
        "# 4. Dự đoán nhãn\n",
        "y_pred_new = clf.predict(X_new)\n",
        "\n",
        "print(\"Dự đoán nhãn cho câu:\", new_sentence[0])\n",
        "print(\"Kết quả:\", y_pred_new[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:54.615912Z",
          "iopub.execute_input": "2025-10-02T12:24:54.616778Z",
          "iopub.status.idle": "2025-10-02T12:24:54.625436Z",
          "shell.execute_reply.started": "2025-10-02T12:24:54.616747Z",
          "shell.execute_reply": "2025-10-02T12:24:54.623865Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlQKETkb531z",
        "outputId": "211eb09d-ec31-4ccb-e9d8-14c499416be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dự đoán nhãn cho câu: best tweet ever!\n",
            "Kết quả: 1\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II. Multinomial Logistic Regression (Multi-class Classification)\n",
        "\n",
        "### Step 1. **Data Loading**\n",
        "- **Dataset**: Import a multi-class dataset (e.g., 20 Newsgroups, or custom dataset with >2 categories)  \n",
        "- **File Access**: Load documents with category labels  \n",
        "- **Initial Inspection**: Show sample text and class distribution  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 2. **Data Preprocessing**\n",
        "- **Text Cleaning**: Lowercasing, punctuation removal  \n",
        "- **Tokenization**: Segment into words  \n",
        "- **Stopword Removal** *(optional)*  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 3. **Feature Representation**\n",
        "- **Bag-of-Words / Count Vectors**: Convert tokens into numerical features  \n",
        "- **Vocabulary**: Build based on training data  \n",
        "- **Matrix Representation**: Documents → rows, words → columns  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 4. **Model Construction**\n",
        "- **Softmax Function**:  \n",
        "  $$ P(y=k|x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^K e^{\\theta_j^T x}} $$\n",
        "- **Loss Function (Multiclass Cross-Entropy)**:  \n",
        "  $$ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K \\mathbf{1}\\{y^{(i)}=k\\}\\log P(y=k|x^{(i)}) $$\n",
        "- **Training**: Fit multinomial logistic regression with gradient descent  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 5. **Evaluation**\n",
        "- **Metrics**: Accuracy, Precision, Recall, F1-score (per class & macro/weighted)  \n",
        "- **Confusion Matrix**: Show classification distribution across classes  \n",
        "- **Error Analysis**: Inspect common misclassifications  \n",
        "\n",
        "---\n",
        "\n",
        "## Required Libraries\n",
        "- **`pandas`, `numpy`**: Data handling  \n",
        "- **`scikit-learn`**: Logistic regression, model evaluation  \n",
        "- **`matplotlib`, `seaborn`**: Visualization  \n",
        "- **`collections`**: Frequency counts  \n"
      ],
      "metadata": {
        "id": "lMx_57MLIEqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ======================\n",
        "# 1. Chọn categories\n",
        "# ======================\n",
        "categories = ['sci.med','rec.sport.hockey', 'sci.space', 'talk.religion.misc', 'comp.graphics']\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    remove=('headers','footers','quotes')\n",
        ")\n",
        "newsgroups_test = fetch_20newsgroups(\n",
        "    subset='test',\n",
        "    categories=categories,\n",
        "    remove=('headers','footers','quotes')\n",
        ")\n",
        "\n",
        "# Gộp train + test lại để chọn subset ~3000\n",
        "all_texts = newsgroups_train.data + newsgroups_test.data\n",
        "all_labels = list(newsgroups_train.target) + list(newsgroups_test.target)\n",
        "\n",
        "# ======================\n",
        "# 2. Lấy ngẫu nhiên 3000 samples\n",
        "# ======================\n",
        "subset_texts, _, subset_labels, _ = train_test_split(\n",
        "    all_texts,\n",
        "    all_labels,\n",
        "    train_size=4000,\n",
        "    stratify=all_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 3. Chia thành train/dev/test (70/15/15)\n",
        "# ======================\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    subset_texts, subset_labels,\n",
        "    test_size=0.3, random_state=42, stratify=subset_labels\n",
        ")\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 4. Tạo DataFrame như code gốc\n",
        "# ======================\n",
        "Topic_data_tr = pd.DataFrame({\"label\": y_train, \"text\": X_train})\n",
        "Topic_data_dev = pd.DataFrame({\"label\": y_dev, \"text\": X_dev})\n",
        "Topic_data_test = pd.DataFrame({\"label\": y_test, \"text\": X_test})\n",
        "\n",
        "# ======================\n",
        "# 5. Đồng thời tạo list + numpy array (format cũ)\n",
        "# ======================\n",
        "topic_tr_textlist = Topic_data_tr[[\"text\"]].values.tolist()\n",
        "ArrayTR_Topic = Topic_data_tr[[\"label\"]].to_numpy()\n",
        "\n",
        "topic_dev_textlist = Topic_data_dev[[\"text\"]].values.tolist()\n",
        "ArrayDev_Topic = Topic_data_dev[[\"label\"]].to_numpy()\n",
        "\n",
        "topic_test_textlist = Topic_data_test[[\"text\"]].values.tolist()\n",
        "ArrayTest_Topic = Topic_data_test[[\"label\"]].to_numpy()\n",
        "\n",
        "# ======================\n",
        "# 6. Kiểm tra\n",
        "# ======================\n",
        "print(\"Train size:\", len(Topic_data_tr))\n",
        "print(\"Dev size:\", len(Topic_data_dev))\n",
        "print(\"Test size:\", len(Topic_data_test))\n",
        "print(\"Số lớp:\", len(np.unique(ArrayTR_Topic)))\n",
        "\n",
        "Topic_data_tr.head()\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:03.212229Z",
          "start_time": "2020-02-15T14:18:03.185261Z"
        },
        "id": "bNDJ6sX0Itlk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:03.761420Z",
          "iopub.execute_input": "2025-10-02T12:25:03.761759Z",
          "iopub.status.idle": "2025-10-02T12:25:05.993383Z",
          "shell.execute_reply.started": "2025-10-02T12:25:03.761735Z",
          "shell.execute_reply": "2025-10-02T12:25:05.992455Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "24562c50-5509-439f-8a6a-ad715f97bb76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 2800\n",
            "Dev size: 600\n",
            "Test size: 600\n",
            "Số lớp: 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                               text\n",
              "0      3  At one time there was speculation that the fir...\n",
              "1      0  I am looking for some fast polygon routines (S...\n",
              "2      1  Can some on e give me some stats on Forsrg in ...\n",
              "3      3  Regarding the feasability of retrieving the HS...\n",
              "4      1  -=> Quoting Greg Rogers to All <=-\\n GR> Hi al..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74e1bbab-8cd6-4b9f-9ddd-2ebba3bbfb9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>At one time there was speculation that the fir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I am looking for some fast polygon routines (S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Can some on e give me some stats on Forsrg in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Regarding the feasability of retrieving the HS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-=&gt; Quoting Greg Rogers to All &lt;=-\\n GR&gt; Hi al...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74e1bbab-8cd6-4b9f-9ddd-2ebba3bbfb9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-74e1bbab-8cd6-4b9f-9ddd-2ebba3bbfb9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-74e1bbab-8cd6-4b9f-9ddd-2ebba3bbfb9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8b7c0004-b9ba-4161-87eb-d930cf15914b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8b7c0004-b9ba-4161-87eb-d930cf15914b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8b7c0004-b9ba-4161-87eb-d930cf15914b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Topic_data_tr",
              "summary": "{\n  \"name\": \"Topic_data_tr\",\n  \"rows\": 2800,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2726,\n        \"samples\": [\n          \"I have just started taking allergy shots a month ago and is \\nstill wondering what I am getting into. A friend of mine told\\nme that the body change every 7 years (whatever that means)\\nand I don't need those antibody-building allergy shots at all.\\nDoes that make sense to anyone?\",\n          \"Oops, what the hell a crosspost is this ?!\\n\\nHave a look onto XV-3.00 before saying anything more about it's power.\\n\",\n          \"#|> #|> \\n#|> #|> #This is quite different from saying \\\"Employing force on other people\\n#|> #|> #is immoral, period.   Unfortunately, from time to time we are obliged\\n#|> #|> #to do this immoral thing for reasons of self-preservation, and so\\n#|> #|> #we have to bear the moral consequences of that.\\n#|> #|> \\n#|> #|> Since both statements, to all intents and purposes, say effectively\\n#|> #|> the same thing, \\n#|> #\\n#|> #Are you serious?  Two statements, one of which says that use of force\\n#|> #in the given situation is moral, and the other of which says it is\\n#|> #not moral \\\"say effectively the same thing?\\\"\\n#|> \\n#|> Yes, when you tag on the \\\"Unfortunately, ...\\\", then to all intents and\\n#|> purposes you are saying the same thing.\\n#\\n#Then delete the \\\"unfortunately\\\".   Now tell me that the two statement\\n#say effectively the same thing.\\n#\\n#And to save everyone a couple of trips round this loop, please notice\\n#that we are only obliged to use force to preserve self.   We can choose\\n#*not* to preserve self, which is the point of pacifism.\\n\\nO.K., got you.  I concede your point, though the word \\\"obliged\\\" strongly\\nimplies that one must sometimes use force.  A further rephrasing would\\ngive you the distinction you mention, however.  If I have you right, a pacifist\\nwould not even go on to say, \\\"unfortunately,etc.\\\"\\n\\n#|> #Would you say this of any two statements, one saying \\\"X is moral\\\" and\\n#|> #the other saying \\\"X is immoral?\\\"   How would you decided when two \\n#|> #statements \\\"X is moral\\\" \\\"X is immoral\\\" actually conflict, and when\\n#|> #they \\\"say effectively the same thing\\\".\\n#|> \\n#|> What they prescribe that one should do is a pretty good indicator.\\n#\\n#And in this case they don't prescribe the same things, so.....\\n\\nYes, fair enough, though why confuse things by saying that \\\"one is \\nsomtimes obliged\\\" if the real meaning is that \\\"one is never obliged\\\".\\n\\n#|> #|>                  and lead one to do precisely the same thing, then \\n#|> #|> either both statements are doublespeak, or none.\\n#|> #\\n#|> #They might lead you to do the same thing, but the difference is what\\n#|> #motivates pacifism so they obviously don't lead pacifists to to the\\n#|> #same thing.\\n#|> \\n#|> That's not true.  You could formulate a pragmatic belief in minimum \\n#|> force and still be a pacifist.  If the minimum is 0, great  - but one is\\n#|> always trying to get as close to 0 force as possible under that belief.\\n#|> Not the same as 'force is immoral, period', but still tending to pacifism.\\n#\\n#If you don't think the use of force is immoral, why minimise its use?\\n\\nIf you don't think that it is \\\"immoral, period.\\\".   \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(topic_test_textlist))"
      ],
      "metadata": {
        "id": "rwV__06LItlk",
        "outputId": "9b4426f5-9b92-4910-c1c6-d7aba169d04f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:11.168380Z",
          "iopub.execute_input": "2025-10-02T12:25:11.168732Z",
          "iopub.status.idle": "2025-10-02T12:25:11.174150Z",
          "shell.execute_reply.started": "2025-10-02T12:25:11.168708Z",
          "shell.execute_reply": "2025-10-02T12:25:11.173061Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "Topic_data_tr.head()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:03.515585Z",
          "start_time": "2020-02-15T14:18:03.508299Z"
        },
        "id": "K3PzDaTjItll",
        "outputId": "13422e36-2838-4e28-caad-bce41fc60127",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:12.910973Z",
          "iopub.execute_input": "2025-10-02T12:25:12.911302Z",
          "iopub.status.idle": "2025-10-02T12:25:12.919993Z",
          "shell.execute_reply.started": "2025-10-02T12:25:12.911277Z",
          "shell.execute_reply": "2025-10-02T12:25:12.919096Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                               text\n",
              "0      3  At one time there was speculation that the fir...\n",
              "1      0  I am looking for some fast polygon routines (S...\n",
              "2      1  Can some on e give me some stats on Forsrg in ...\n",
              "3      3  Regarding the feasability of retrieving the HS...\n",
              "4      1  -=> Quoting Greg Rogers to All <=-\\n GR> Hi al..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a62170b-b8bd-4578-83e7-8704d4516ad2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>At one time there was speculation that the fir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I am looking for some fast polygon routines (S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Can some on e give me some stats on Forsrg in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Regarding the feasability of retrieving the HS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-=&gt; Quoting Greg Rogers to All &lt;=-\\n GR&gt; Hi al...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a62170b-b8bd-4578-83e7-8704d4516ad2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a62170b-b8bd-4578-83e7-8704d4516ad2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a62170b-b8bd-4578-83e7-8704d4516ad2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-13ce311c-97b9-4b1d-abab-1ca6ca191b12\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13ce311c-97b9-4b1d-abab-1ca6ca191b12')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-13ce311c-97b9-4b1d-abab-1ca6ca191b12 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Topic_data_tr",
              "summary": "{\n  \"name\": \"Topic_data_tr\",\n  \"rows\": 2800,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2726,\n        \"samples\": [\n          \"I have just started taking allergy shots a month ago and is \\nstill wondering what I am getting into. A friend of mine told\\nme that the body change every 7 years (whatever that means)\\nand I don't need those antibody-building allergy shots at all.\\nDoes that make sense to anyone?\",\n          \"Oops, what the hell a crosspost is this ?!\\n\\nHave a look onto XV-3.00 before saying anything more about it's power.\\n\",\n          \"#|> #|> \\n#|> #|> #This is quite different from saying \\\"Employing force on other people\\n#|> #|> #is immoral, period.   Unfortunately, from time to time we are obliged\\n#|> #|> #to do this immoral thing for reasons of self-preservation, and so\\n#|> #|> #we have to bear the moral consequences of that.\\n#|> #|> \\n#|> #|> Since both statements, to all intents and purposes, say effectively\\n#|> #|> the same thing, \\n#|> #\\n#|> #Are you serious?  Two statements, one of which says that use of force\\n#|> #in the given situation is moral, and the other of which says it is\\n#|> #not moral \\\"say effectively the same thing?\\\"\\n#|> \\n#|> Yes, when you tag on the \\\"Unfortunately, ...\\\", then to all intents and\\n#|> purposes you are saying the same thing.\\n#\\n#Then delete the \\\"unfortunately\\\".   Now tell me that the two statement\\n#say effectively the same thing.\\n#\\n#And to save everyone a couple of trips round this loop, please notice\\n#that we are only obliged to use force to preserve self.   We can choose\\n#*not* to preserve self, which is the point of pacifism.\\n\\nO.K., got you.  I concede your point, though the word \\\"obliged\\\" strongly\\nimplies that one must sometimes use force.  A further rephrasing would\\ngive you the distinction you mention, however.  If I have you right, a pacifist\\nwould not even go on to say, \\\"unfortunately,etc.\\\"\\n\\n#|> #Would you say this of any two statements, one saying \\\"X is moral\\\" and\\n#|> #the other saying \\\"X is immoral?\\\"   How would you decided when two \\n#|> #statements \\\"X is moral\\\" \\\"X is immoral\\\" actually conflict, and when\\n#|> #they \\\"say effectively the same thing\\\".\\n#|> \\n#|> What they prescribe that one should do is a pretty good indicator.\\n#\\n#And in this case they don't prescribe the same things, so.....\\n\\nYes, fair enough, though why confuse things by saying that \\\"one is \\nsomtimes obliged\\\" if the real meaning is that \\\"one is never obliged\\\".\\n\\n#|> #|>                  and lead one to do precisely the same thing, then \\n#|> #|> either both statements are doublespeak, or none.\\n#|> #\\n#|> #They might lead you to do the same thing, but the difference is what\\n#|> #motivates pacifism so they obviously don't lead pacifists to to the\\n#|> #same thing.\\n#|> \\n#|> That's not true.  You could formulate a pragmatic belief in minimum \\n#|> force and still be a pacifist.  If the minimum is 0, great  - but one is\\n#|> always trying to get as close to 0 force as possible under that belief.\\n#|> Not the same as 'force is immoral, period', but still tending to pacifism.\\n#\\n#If you don't think the use of force is immoral, why minimise its use?\\n\\nIf you don't think that it is \\\"immoral, period.\\\".   \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "topic_vocab, topic_df, topic_ngram_counts_tr = get_vocab(topic_tr_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
        "#topic_vocab, topic_df_dev, topic_ngram_counts_dev = get_vocab(topic_dev_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
        "#topic_vocab,topic_df_test,topic_ngram_counts_test = get_vocab(topic_test_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:03.806523Z",
          "start_time": "2020-02-15T14:18:03.798279Z"
        },
        "id": "PJxxIxHMItlm",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:14.963327Z",
          "iopub.execute_input": "2025-10-02T12:25:14.963670Z",
          "iopub.status.idle": "2025-10-02T12:25:37.740708Z",
          "shell.execute_reply.started": "2025-10-02T12:25:14.963644Z",
          "shell.execute_reply": "2025-10-02T12:25:37.739678Z"
        }
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_vocab = list(topic_vocab)\n",
        "vocab_id_topic = {i:list_of_vocab[i] for i in range(len(topic_vocab))}"
      ],
      "metadata": {
        "id": "xks459SjItln",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:43.386972Z",
          "iopub.execute_input": "2025-10-02T12:25:43.387297Z",
          "iopub.status.idle": "2025-10-02T12:25:43.393812Z",
          "shell.execute_reply.started": "2025-10-02T12:25:43.387271Z",
          "shell.execute_reply": "2025-10-02T12:25:43.392673Z"
        }
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(topic_vocab))\n",
        "print(list(topic_vocab)[:100])\n",
        "print()\n",
        "print(topic_df.most_common()[:10])\n",
        "#print(list_of_vocab[:10])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:04.508938Z",
          "start_time": "2020-02-15T14:18:04.171071Z"
        },
        "scrolled": true,
        "id": "v-x8qDnWItlw",
        "outputId": "dbe18852-2eb6-4e74-e8e7-3aa58dd199f5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:46.139875Z",
          "iopub.execute_input": "2025-10-02T12:25:46.140226Z",
          "iopub.status.idle": "2025-10-02T12:25:46.256760Z",
          "shell.execute_reply.started": "2025-10-02T12:25:46.140200Z",
          "shell.execute_reply": "2025-10-02T12:25:46.255685Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "['shareware', 'Sun', 'Colorado', 'depth', 'nanother', 'permanent', 'using', 'Habs', 'In fact', 'visit', 'all sorts', 'Stanford', 'chronic', 'turned', 'admit', 'fitting', 'nproblem', 'posted', 'reason', 'every', 'bother', 'Description', 'assist', 'Guide', 'Engineering', 'solar', 'Macintosh', 'Hall', 'themselves', 'owned', 'aware', 'bringing', 'It would', 'Goal', 'Much', 'ndon', 'rights', 'burden', 'bacteria', 'referring', 'nMark', 'flying', 'manned', 'large', 'yet', 'platform', 'Otto', 'explains', 'berkeley', 'things', 'scale', 'skin', 'raw', 'daughter', 'wisc', 'exposure', 'parents', 'News', 'asking', 'least', 'if re', 'magnetic', 'studies', 'puts', 'much', 'arguments', 'vote', 'second period', 'cares', 'but still', 'not good', 'Pluto', 'preventing', 'comp', 'me some', 'binary', 'Sharks', 'during', 'server', 'rendering', 'easily', 'nonsense', 'automatically', 'Dryden', 'condition', 'ngeb cadre', 'cadre', 'Matthew', 'terminal', 'designed', 'detail', 'same way', 'if don', 'mit edu', 'andrew cmu', 'unc', 'Earth', 'sound', 'Management', 'if not']\n",
            "\n",
            "[('not', 1029), ('but', 960), ('would', 776), ('if', 764), ('about', 744), ('nI', 730), ('one', 649), ('all', 624), ('some', 614), ('out', 613)]\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": [
        "##Counts\n",
        "\n",
        "topic_X_tr_ngram = Extract_X_ngram(topic_tr_textlist)\n",
        "topic_X_tr_count = vectorise(topic_X_tr_ngram,topic_vocab)\n",
        "\n",
        "topic_XDev_ngram = Extract_X_ngram(topic_dev_textlist)\n",
        "topic_X_dev_count = vectorise(topic_XDev_ngram,topic_vocab)\n",
        "\n",
        "topic_XTest_ngram = Extract_X_ngram(topic_test_textlist)\n",
        "topic_X_test_count = vectorise(topic_XTest_ngram,topic_vocab)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:04.706802Z",
          "start_time": "2020-02-15T14:18:04.511061Z"
        },
        "id": "RnBQSkOaItlx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:48.693181Z",
          "iopub.execute_input": "2025-10-02T12:25:48.693483Z",
          "iopub.status.idle": "2025-10-02T12:26:34.492833Z",
          "shell.execute_reply.started": "2025-10-02T12:25:48.693462Z",
          "shell.execute_reply": "2025-10-02T12:26:34.491698Z"
        }
      },
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 1. Train với Count Vectors\n",
        "# ======================\n",
        "sgd_count = SGDClassifier(\n",
        "    loss=\"log_loss\",      # logistic regression\n",
        "    penalty=\"l2\",         # regularization\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "sgd_count.fit(topic_X_tr_count, ArrayTR_Topic.ravel())\n",
        "\n",
        "# Evaluate trên Dev set\n",
        "y_dev_pred_count = sgd_count.predict(topic_X_dev_count)\n",
        "print(\"=== Logistic Regression (Count Vectors) ===\")\n",
        "print(classification_report(ArrayDev_Topic, y_dev_pred_count))\n",
        "print(confusion_matrix(ArrayDev_Topic, y_dev_pred_count))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:28:07.593311Z",
          "iopub.execute_input": "2025-10-02T12:28:07.593669Z",
          "iopub.status.idle": "2025-10-02T12:28:10.685209Z",
          "shell.execute_reply.started": "2025-10-02T12:28:07.593643Z",
          "shell.execute_reply": "2025-10-02T12:28:10.683958Z"
        },
        "id": "B9RRUwxa5312",
        "outputId": "3ee7e99a-5273-4aa0-b564-5998dc12bc14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Logistic Regression (Count Vectors) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       127\n",
            "           1       0.87      0.85      0.86       131\n",
            "           2       0.76      0.82      0.79       130\n",
            "           3       0.82      0.74      0.78       129\n",
            "           4       0.74      0.76      0.75        83\n",
            "\n",
            "    accuracy                           0.81       600\n",
            "   macro avg       0.81      0.81      0.81       600\n",
            "weighted avg       0.81      0.81      0.81       600\n",
            "\n",
            "[[110   3   9   3   2]\n",
            " [  1 111   8   5   6]\n",
            " [ 10   1 106   7   6]\n",
            " [  7   5  13  96   8]\n",
            " [  2   8   4   6  63]]\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_count = sgd_count.predict(topic_X_test_count)\n",
        "\n",
        "print(\"\\n=== Logistic Regression (Count Vectors) - Test set ===\")\n",
        "print(\"Classification Report (Test set):\")\n",
        "print(classification_report(ArrayTest_Topic, y_test_pred_count))\n",
        "print(\"Confusion Matrix (Test set):\")\n",
        "print(confusion_matrix(ArrayTest_Topic, y_test_pred_count))\n",
        "print(\"Accuracy:\", accuracy_score(ArrayTest_Topic, y_test_pred_count))\n",
        "print(\"Precision:\", precision_score(ArrayTest_Topic, y_test_pred_count, average='macro'))\n",
        "print(\"Recall:\", recall_score(ArrayTest_Topic, y_test_pred_count, average='macro'))\n",
        "print(\"F1-Score:\", f1_score(ArrayTest_Topic, y_test_pred_count, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2oT8YA-NHWy",
        "outputId": "52a151ea-d47e-43c0-8f2f-dca9b8c94c5d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression (Count Vectors) - Test set ===\n",
            "Classification Report (Test set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.82       128\n",
            "           1       0.87      0.80      0.83       131\n",
            "           2       0.71      0.84      0.77       129\n",
            "           3       0.78      0.72      0.75       130\n",
            "           4       0.69      0.65      0.67        82\n",
            "\n",
            "    accuracy                           0.78       600\n",
            "   macro avg       0.77      0.77      0.77       600\n",
            "weighted avg       0.78      0.78      0.78       600\n",
            "\n",
            "Confusion Matrix (Test set):\n",
            "[[106   4  10   5   3]\n",
            " [  2 105   7   5  12]\n",
            " [  6   3 108   6   6]\n",
            " [ 12   4  17  94   3]\n",
            " [  3   5  10  11  53]]\n",
            "Accuracy: 0.7766666666666666\n",
            "Precision: 0.773034305926261\n",
            "Recall: 0.7672558812748782\n",
            "F1-Score: 0.7685179963641011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Input mới\n",
        "new_sentence = [\"Astronomers discovered a new exoplanet orbiting a distant star, raising hopes for extraterrestrial life\"]\n",
        "\n",
        "# 2. N-gram hóa (giống lúc train)\n",
        "new_sentence_ngram = Extract_X_ngram(new_sentence)\n",
        "\n",
        "# 3. Vectorize với vocab cũ\n",
        "X_new_count = vectorise(new_sentence_ngram, list_of_vocab)  # Count vector\n",
        "\n",
        "# 4. Predict với mô hình Count\n",
        "y_pred_new_count = sgd_count.predict(X_new_count)\n",
        "\n",
        "print(\"Câu:\", new_sentence[0])\n",
        "print(\"Dự đoán (Count):\", y_pred_new_count[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:28:17.231421Z",
          "iopub.execute_input": "2025-10-02T12:28:17.231767Z",
          "iopub.status.idle": "2025-10-02T12:28:17.242361Z",
          "shell.execute_reply.started": "2025-10-02T12:28:17.231743Z",
          "shell.execute_reply": "2025-10-02T12:28:17.241075Z"
        },
        "id": "FlNmVnKx5313",
        "outputId": "00445190-bdfa-41de-e43c-41fae18c6682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Câu: Astronomers discovered a new exoplanet orbiting a distant star, raising hopes for extraterrestrial life\n",
            "Dự đoán (Count): 3\n"
          ]
        }
      ],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "# Lấy lại categories dùng khi load dataset\n",
        "categories = ['sci.med','rec.sport.hockey', 'sci.space', 'talk.religion.misc', 'comp.graphics']\n",
        "\n",
        "# Load dataset train\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    remove=('headers','footers','quotes')\n",
        ")\n",
        "\n",
        "# In các class số\n",
        "print(\"Các class số:\", np.unique(newsgroups_train.target))\n",
        "\n",
        "# In mapping class số -> label\n",
        "for i, name in enumerate(newsgroups_train.target_names):\n",
        "    print(f\"{i} → {name}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:28:19.969072Z",
          "iopub.execute_input": "2025-10-02T12:28:19.969372Z",
          "iopub.status.idle": "2025-10-02T12:28:21.299407Z",
          "shell.execute_reply.started": "2025-10-02T12:28:19.969351Z",
          "shell.execute_reply": "2025-10-02T12:28:21.298540Z"
        },
        "id": "CF4NjYMF5313",
        "outputId": "1ea2c293-49ff-4508-c9fc-c781207483a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Các class số: [0 1 2 3 4]\n",
            "0 → comp.graphics\n",
            "1 → rec.sport.hockey\n",
            "2 → sci.med\n",
            "3 → sci.space\n",
            "4 → talk.religion.misc\n"
          ]
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wi12SjreNOxg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
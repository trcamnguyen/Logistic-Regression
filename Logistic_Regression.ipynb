{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 4140,
          "sourceType": "datasetVersion",
          "datasetId": 2477
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trcamnguyen/Logistic-Regression/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/repo-name/blob/main/notebook.ipynb)\n"
      ],
      "metadata": {
        "id": "qcwrCbfQ9y3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression for Text Classification\n",
        "\n",
        "## Overview\n",
        "This notebook implements and evaluates **Logistic Regression models** for text classification tasks:\n",
        "- **Binary Logistic Regression**: Sentiment analysis (positive vs negative)\n",
        "- **Multinomial Logistic Regression**: Multi-class text classification (softmax regression)\n",
        "\n",
        "## Objectives\n",
        "- Build Logistic Regression models for both binary and multi-class text data\n",
        "- Understand differences between binary vs multinomial logistic regression\n",
        "- Evaluate model quality with accuracy, precision, recall, F1-score, and confusion matrix\n",
        "\n",
        "---\n",
        "\n",
        "# Part I. Binary Logistic Regression (Sentiment Analysis)\n",
        "\n",
        "### Step 1. **Data Loading**\n",
        "- **Dataset**: Import sentiment dataset (e.g., Sentiment140 or custom text dataset)\n",
        "- **File Access**: Load text samples and corresponding binary sentiment labels\n",
        "- **Initial Inspection**: Preview data distribution (positive vs negative)\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2. **Data Preprocessing**\n",
        "- **Text Cleaning**: Remove punctuation, digits, and special characters  \n",
        "- **Normalization**: Convert to lowercase  \n",
        "- **Tokenization**: Split text into words or tokens  \n",
        "- **Stopword Removal** *(optional)*  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 3. **Feature Representation**\n",
        "- **Bag-of-Words**: Convert tokenized text into numerical feature vectors  \n",
        "- **Vocabulary Construction**: Build word-index mapping  \n",
        "- **Binary/Count Representation**: Represent presence or frequency of terms  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 4. **Model Construction**\n",
        "- **Hypothesis Function**:  \n",
        "  $$ h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}} $$\n",
        "- **Loss Function (Binary Cross-Entropy)**:  \n",
        "  $$ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\Big[y^{(i)}\\log h_\\theta(x^{(i)}) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\Big] $$\n",
        "- **Training**: Fit logistic regression classifier on binary labels  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 5. **Evaluation**\n",
        "- **Metrics**: Accuracy, Precision, Recall, F1-score  \n",
        "- **Confusion Matrix**: Visualize predictions vs ground truth  \n",
        "- **Error Analysis**: Inspect misclassified examples  \n",
        "\n",
        "---\n",
        "\n",
        "# Part II. Multinomial Logistic Regression (Multi-class Classification)\n",
        "\n",
        "### Step 1. **Data Loading**\n",
        "- **Dataset**: Import a multi-class dataset (e.g., 20 Newsgroups, or custom dataset with >2 categories)  \n",
        "- **File Access**: Load documents with category labels  \n",
        "- **Initial Inspection**: Show sample text and class distribution  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 2. **Data Preprocessing**\n",
        "- **Text Cleaning**: Lowercasing, punctuation removal  \n",
        "- **Tokenization**: Segment into words  \n",
        "- **Stopword Removal** *(optional)*  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 3. **Feature Representation**\n",
        "- **Bag-of-Words / Count Vectors**: Convert tokens into numerical features  \n",
        "- **Vocabulary**: Build based on training data  \n",
        "- **Matrix Representation**: Documents → rows, words → columns  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 4. **Model Construction**\n",
        "- **Softmax Function**:  \n",
        "  $$ P(y=k|x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^K e^{\\theta_j^T x}} $$\n",
        "- **Loss Function (Multiclass Cross-Entropy)**:  \n",
        "  $$ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K \\mathbf{1}\\{y^{(i)}=k\\}\\log P(y=k|x^{(i)}) $$\n",
        "- **Training**: Fit multinomial logistic regression with gradient descent  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 5. **Evaluation**\n",
        "- **Metrics**: Accuracy, Precision, Recall, F1-score (per class & macro/weighted)  \n",
        "- **Confusion Matrix**: Show classification distribution across classes  \n",
        "- **Error Analysis**: Inspect common misclassifications  \n",
        "\n",
        "---\n",
        "\n",
        "## Required Libraries\n",
        "- **`pandas`, `numpy`**: Data handling  \n",
        "- **`scikit-learn`**: Logistic regression, model evaluation  \n",
        "- **`matplotlib`, `seaborn`**: Visualization  \n",
        "- **`collections`**: Frequency counts  \n",
        "\n",
        "---\n",
        "\n",
        "## Expected Outcomes\n",
        "- **Binary Logistic Regression**: Effective sentiment classifier with clear positive/negative separation  \n",
        "- **Multinomial Logistic Regression**: Generalized model for multi-class problems using softmax  \n",
        "- **Evaluation Metrics**: Demonstrated on both binary and multi-class datasets  \n",
        "- **Insights**: Comparison of logistic regression behavior in binary vs multinomial settings  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0-1GmK0gD8sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from scipy.special import softmax as sftmx #used to test my function, allowed\n",
        "# fixing random seed for reproducibility\n",
        "random.seed(123)\n",
        "np.random.seed(123)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:31:36.292691Z",
          "start_time": "2020-02-15T14:31:35.549108Z"
        },
        "id": "tfe_F1IAItkk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:23:58.691471Z",
          "iopub.execute_input": "2025-10-02T12:23:58.692180Z",
          "iopub.status.idle": "2025-10-02T12:23:58.698008Z",
          "shell.execute_reply.started": "2025-10-02T12:23:58.692151Z",
          "shell.execute_reply": "2025-10-02T12:23:58.696870Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Raw texts and labels into arrays\n",
        "\n",
        "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
      ],
      "metadata": {
        "id": "OdgfwX6pItkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "# Download Sentiment140\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "file_path = path + \"/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "cols = ['target','id','date','flag','user','text']\n",
        "df = pd.read_csv(file_path, encoding='ISO-8859-1', names=cols)\n",
        "\n",
        "# Giữ lại text + label, đổi 4 -> 1\n",
        "df = df[['text','target']]\n",
        "df['target'] = df['target'].replace(4,1)\n",
        "\n",
        "# Lấy subset nhỏ gọn 10,000 dòng\n",
        "df_small = df.sample(n=10000, random_state=42)\n",
        "\n",
        "# Chia train/dev/test\n",
        "train_df, test_df = train_test_split(df_small, test_size=0.2, random_state=42, stratify=df_small['target'])\n",
        "train_df, dev_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['target'])\n",
        "\n",
        "# Format\n",
        "data_tr_textlist = train_df[['text']].values.tolist()\n",
        "ArrayTR = train_df[['target']].to_numpy()\n",
        "\n",
        "data_dev_textlist = dev_df[['text']].values.tolist()\n",
        "ArrayDev = dev_df[['target']].to_numpy()\n",
        "\n",
        "data_test_textlist = test_df[['text']].values.tolist()\n",
        "ArrayTest = test_df[['target']].to_numpy()\n",
        "\n",
        "print(\"Binary dataset size:\", len(df_small))\n",
        "print(\"Train/Dev/Test:\", len(data_tr_textlist), len(data_dev_textlist), len(data_test_textlist))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyd1BBp1Itkp",
        "outputId": "80afe6b6-00be-4200-a15e-35de460d6a10",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:02.361903Z",
          "iopub.execute_input": "2025-10-02T12:24:02.362768Z",
          "iopub.status.idle": "2025-10-02T12:24:07.176997Z",
          "shell.execute_reply.started": "2025-10-02T12:24:02.362738Z",
          "shell.execute_reply": "2025-10-02T12:24:07.176011Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'sentiment140' dataset.\n",
            "Binary dataset size: 10000\n",
            "Train/Dev/Test: 7200 800 2000\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag-of-Words Representation\n",
        "\n",
        "\n",
        "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
        "\n",
        "\n",
        "## Text Pre-Processing Pipeline\n",
        "\n",
        "To obtain a vocabulary of features, you should:\n",
        "- tokenise all texts into a list of unigrams (tip: using a regular expression)\n",
        "- remove stop words (using the one provided or one of your preference)\n",
        "- compute bigrams, trigrams given the remaining unigrams\n",
        "- remove ngrams appearing in less than K documents\n",
        "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (you can keep top N if you encounter memory issues).\n"
      ],
      "metadata": {
        "id": "OmktummkItkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = ['a','in','on','at','and','or',\n",
        "              'to', 'the', 'of', 'an', 'by',\n",
        "              'as', 'is', 'was', 'were', 'been', 'be',\n",
        "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
        "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
        "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
        "             'his', 'her', 'they', 'them', 'from', 'with', 'its','also','so','there','their','The']"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:31.860420Z",
          "start_time": "2020-02-15T14:17:31.855439Z"
        },
        "id": "HRspqqpRItkt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:09.227122Z",
          "iopub.execute_input": "2025-10-02T12:24:09.227443Z",
          "iopub.status.idle": "2025-10-02T12:24:09.234062Z",
          "shell.execute_reply.started": "2025-10-02T12:24:09.227418Z",
          "shell.execute_reply": "2025-10-02T12:24:09.232544Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gram extraction from a document\n",
        "\n",
        "You first need to implement the `extract_ngrams` function. It takes as input:\n",
        "- `x_raw`: a string corresponding to the raw text of a document\n",
        "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
        "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
        "- `stop_words`: a list of stop words\n",
        "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
        "\n",
        "and returns:\n",
        "\n",
        "- a list of all extracted features.\n",
        "\n",
        "See the examples below to see how this function should work."
      ],
      "metadata": {
        "id": "lUOnMJC_Itkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', stop_words= stop_words, vocab=set()):\n",
        "\n",
        "\n",
        "    ####Tokenisation#######\n",
        "    com = re.compile(token_pattern)\n",
        "    x_raw = com.findall(x_raw)\n",
        "    # vocab = re.findall(token_pattern,vocab)\n",
        "\n",
        "   # x_raw = list(x_raw.split())     #Split Sentence into seperate words then store as list\n",
        "\n",
        "\n",
        " #####Remove Stop words########\n",
        "    x_raw=[word for word in x_raw if word not in stop_words]\n",
        "    for word in stop_words:\n",
        "        for word2 in x_raw:\n",
        "            if word == word2:\n",
        "                x_raw.remove(word)\n",
        "\n",
        "\n",
        "\n",
        "    #print(vocab)\n",
        "\n",
        "#######Return Vocab#####\n",
        "    #vocab needs to normalised\n",
        "    vocab1 = str(vocab)\n",
        "    vocab1 = re.findall(token_pattern,vocab1)\n",
        "    commachar = \",\"\n",
        "    spacechar = ''\n",
        "    if ((len(vocab1)) != 0): #Check if set is empty\n",
        "        vocablist = []\n",
        "        for word in x_raw:\n",
        "            for word2 in vocab1:\n",
        "                if ((word == word2) & (word != commachar) & (word != spacechar)& (word2 not in vocablist)): #remove commas & match words in text &vocab\n",
        "                    vocablist.append(word.replace(\" \", \"\")) ## remove whitespace characters\n",
        "                   # print(vocablist)\n",
        "\n",
        "    noofngrams=[]\n",
        "    ngrams_list = []\n",
        "    if ngram_range == (1,3):\n",
        "        noofngrams= [1,2,3]\n",
        "    if ngram_range == (1,2):\n",
        "        noofngrams= [1,2]\n",
        "\n",
        "    y =[]\n",
        "\n",
        "    #Extract Ngrams\n",
        "    for n in noofngrams:\n",
        "        if (len(vocab)) == 0:\n",
        "                for num in range(0, len(x_raw)):\n",
        "                    ngram = ' '.join(x_raw[num:num + n])\n",
        "                    #if ngram not in ngrams_list:\n",
        "                    if ((ngram != '')):\n",
        "                        ngrams_list.append(ngram)\n",
        "                        if ngram not in y:\n",
        "                            y.append(ngram)\n",
        "\n",
        "        if (len(vocab)) != 0:\n",
        "                for n in range(0,len(vocab1)):\n",
        "                    for num in range(0, len(vocablist)):\n",
        "                        if len(vocablist[num]) != 0:\n",
        "                                ngram = ' '.join(vocablist[num:num + n])\n",
        "\n",
        "                                if ((ngram != '')): # ((ngram not in ngrams_list) &\n",
        "                                    ngrams_list.append(ngram)\n",
        "\n",
        "                                    if ngram not in y:\n",
        "                                        y.append(ngram)\n",
        "\n",
        "    x= ngrams_list\n",
        "    return x,y # y is unique ngram list, x is list including duplicates for vectorisation"
      ],
      "metadata": {
        "id": "CrI3-VziItku",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:11.865374Z",
          "iopub.execute_input": "2025-10-02T12:24:11.865715Z",
          "iopub.status.idle": "2025-10-02T12:24:11.877557Z",
          "shell.execute_reply.started": "2025-10-02T12:24:11.865692Z",
          "shell.execute_reply": "2025-10-02T12:24:11.876617Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "x1,y1 = extract_ngrams(\"this is a great movie to watch\",\n",
        "               ngram_range=(1,3),\n",
        "               stop_words=stop_words)\n",
        "print(y1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:33.680114Z",
          "start_time": "2020-02-15T14:17:33.675339Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmbRpQasItkw",
        "outputId": "1abf50a7-935e-4aed-e4b9-2870538b4ba5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:14.774636Z",
          "iopub.execute_input": "2025-10-02T12:24:14.774969Z",
          "iopub.status.idle": "2025-10-02T12:24:14.780949Z",
          "shell.execute_reply.started": "2025-10-02T12:24:14.774949Z",
          "shell.execute_reply": "2025-10-02T12:24:14.779919Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['great', 'movie', 'watch', 'great movie', 'movie watch', 'great movie watch']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
      ],
      "metadata": {
        "id": "jWYnA9xOItky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a vocabulary of n-grams\n",
        "\n",
        "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
        "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
        "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
        "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
        "- `stop_words`: a list of stop words\n",
        "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
        "- `min_df`: keep ngrams with a minimum document frequency.\n",
        "- `keep_topN`: keep top-N more frequent ngrams.\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `vocab`: a set of the n-grams that will be used as features.\n",
        "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
        "- `ngram_counts`: counts of each ngram in vocab\n",
        "\n",
        "Hint: it should make use of the `extract_ngrams` function."
      ],
      "metadata": {
        "id": "8un6kwCMItkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=0, stop_words=[]):\n",
        "\n",
        "    #Define empty placeholder variables\n",
        "    df=  Counter()\n",
        "    ngram_counts = Counter()\n",
        "    vocab = set()\n",
        "    df1 = list()\n",
        "    df2 = Counter()\n",
        "    #com = re.compile(token_pattern)\n",
        "    #X_raw = com.findall(X_raw)\n",
        "    vocab1 = set()\n",
        "\n",
        "    for x_raw in X_raw:\n",
        "        x_raw = str(x_raw)\n",
        "        com = re.compile(token_pattern)\n",
        "        x_raw = com.findall(x_raw)\n",
        "        x_raw = str(x_raw)\n",
        "        x_raw.replace(\" \", \"\")\n",
        "        x_raw.replace(\"[\",\"\")\n",
        "        x_raw.replace(\"]\",\"\")\n",
        "        y,x = extract_ngrams(x_raw, ngram_range, token_pattern, stop_words)\n",
        "        #calling extract_ngrams deals with tokenisation,stop words,ngram range and ngram extraction\n",
        "        for item in x:\n",
        "            df[item] += 1\n",
        "            if df[item]> min_df: # Only keep above the minimum\n",
        "                df2[item] += 1\n",
        "\n",
        "            if (str(item) not in vocab) & (len(vocab) < keep_topN): #stop adding to vocab once cap is reached\n",
        "            #vocab.add(str(x))\n",
        "                 vocab.add(str(item)) #need to remove brackets as i think its passing\n",
        "\n",
        "        for string in x:\n",
        "            if string in ngram_counts:\n",
        "                    ngram_counts[string] += 1 #len 998359\n",
        "            else:\n",
        "                    ngram_counts[string] = 1\n",
        "    #df1.append(df2.most_common(keep_topN)) #Keep only the specified amount of N\n",
        "    #ngram_counts.append(ngram_counts.most_common(keep_topN))\n",
        "    #vocab = list(vocab)\n",
        "    df = df2\n",
        "\n",
        "\n",
        "#######assign top ngrams to vocab set###########################\n",
        "   # vocab1 = set()\n",
        "    for item in ngram_counts.most_common(keep_topN):\n",
        "\n",
        "            # type(item)) item is a tupple\n",
        "        if (len(vocab1) < keep_topN)  :\n",
        "                vocab1.add(item[0])\n",
        "\n",
        "    vocab = vocab1\n",
        "    return vocab, df, ngram_counts"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:35.821240Z",
          "start_time": "2020-02-15T14:17:35.814722Z"
        },
        "id": "nfhxA8jEItk0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:17.325291Z",
          "iopub.execute_input": "2025-10-02T12:24:17.325640Z",
          "iopub.status.idle": "2025-10-02T12:24:17.334305Z",
          "shell.execute_reply.started": "2025-10-02T12:24:17.325606Z",
          "shell.execute_reply": "2025-10-02T12:24:17.333276Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
      ],
      "metadata": {
        "id": "dYwdXNWZItk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, df, ngram_counts = get_vocab(data_tr_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
        "print(len(vocab))\n",
        "print()\n",
        "print(list(vocab)[:100])\n",
        "print()\n",
        "print(df.most_common()[:10])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:39.319793Z",
          "start_time": "2020-02-15T14:17:36.836545Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BCE_lc-Itk1",
        "outputId": "66652b53-3091-4aaf-bdd4-5472ac3a5b2f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:20.217581Z",
          "iopub.execute_input": "2025-10-02T12:24:20.217925Z",
          "iopub.status.idle": "2025-10-02T12:24:21.221749Z",
          "shell.execute_reply.started": "2025-10-02T12:24:20.217902Z",
          "shell.execute_reply": "2025-10-02T12:24:21.220789Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "\n",
            "['Today', 'hand', 'why many', 'medical', 'Safari', 'happen', 'signing', 'apt', 'Williams', 'summer', 'School', 'Max', 'happening', 'love new', 'now no', 'addicted', 'cms', 'today wish', 'got work', 'favorite', 'god', 'excited about', 'turn', 'not feeling', 'mention', 'presentation', 'Not looking', 'Makes me', 'getting old', 'welcome Twitter', 'use', 'angels', 'lost Please help', 'dinner', 'prepare', 'worry about', 'key', 'radio', 'bitch', 'looking forward week', 'finger', 'since almost', 'school tomorrow', 'make me', 'bath', 'one but', 'view', 'step', 'but only', 'like re', 'school quot', 'hanging', 'no luck', 're back', 'An', 'shame', 'me sad', 'bright', 'up where', 'my night', 'Actually', 'hold', 'make', 'They', 'final', 'post', 'time years', 'Of course', 'great', 'Too bad', 'slumber', 'Season', 'london', 'read', 'peeps', 'Oprah', 'shelter', 'Aye', 'Ahhh', 'feel bad', 'really really', 'ASAP', 'up early', 'guy', 'features', 'prefer', 'awesome', 'www tweeterfollow com', 'get into', 'my friend', 'aren', 'Week', 're going', 'Station', 'technology', 've', 'Get followers', 'Thinking about', 'me anymore', 'heels']\n",
            "\n",
            "[('my', 1124), ('me', 640), ('but', 499), ('not', 419), ('just', 416), ('up', 378), ('get', 340), ('good', 339), ('now', 332), ('all', 332)]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, you need to create vocabulary id -> word and id -> word dictionaries for reference:"
      ],
      "metadata": {
        "id": "aDUK6EtxItk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_vocab = list(vocab)\n",
        "vocab_id = {i:list_of_vocab[i] for i in range(len(vocab))}"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:39.326811Z",
          "start_time": "2020-02-15T14:17:39.322256Z"
        },
        "id": "-eU1uRMgItk2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:23.668870Z",
          "iopub.execute_input": "2025-10-02T12:24:23.669167Z",
          "iopub.status.idle": "2025-10-02T12:24:23.676168Z",
          "shell.execute_reply.started": "2025-10-02T12:24:23.669145Z",
          "shell.execute_reply": "2025-10-02T12:24:23.675225Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you should be able to extract n-grams for each text in the training, development and test sets:"
      ],
      "metadata": {
        "id": "i6LEY9FnItk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorise documents"
      ],
      "metadata": {
        "id": "islhfNHEItk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
        "- `X_ngram`: a list of texts (documents), where each text (doc) is represented as list of n-grams in the `vocab`\n",
        "- `vocab`: a set of n-grams to be used for representing the documents\n",
        "\n",
        "and return:\n",
        "- `X_vec`: an array with dimensionality N x |vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
      ],
      "metadata": {
        "id": "Mw8BuNP2Itk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Take training set as input\n",
        "#divide into doc\n",
        "#extract each ngram for each doc\n",
        "#Count ngram in doc using counter\n",
        "#iterate counter through the vocab and assign counts in that order\n",
        "def Extract_X_ngram(data_tr_textlist):\n",
        "    X_ngram =[]\n",
        "    for line in data_tr_textlist:\n",
        "        ngram,p = list(extract_ngrams(str(line),stop_words= stop_words))\n",
        "        X_ngram.append(ngram)\n",
        "    return X_ngram\n",
        "\n",
        "X_ngram = Extract_X_ngram(data_tr_textlist)\n",
        "#print(Counter(X_ngram[0]))"
      ],
      "metadata": {
        "id": "c9qF2QbIItk4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:25.840387Z",
          "iopub.execute_input": "2025-10-02T12:24:25.840766Z",
          "iopub.status.idle": "2025-10-02T12:24:26.191870Z",
          "shell.execute_reply.started": "2025-10-02T12:24:25.840728Z",
          "shell.execute_reply": "2025-10-02T12:24:26.190963Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorise(X_ngram, vocab):\n",
        "    N = len(X_ngram) #Get Dimensions #1400 x 5000 for training set\n",
        "    SizeOfSet = len(vocab)\n",
        "    X_vec = np.zeros((N,SizeOfSet))\n",
        "    counter_vect = Counter()\n",
        "    i = 0\n",
        "    temp_loc =0\n",
        "\n",
        "    for row in X_ngram:\n",
        "        counter_vect = Counter(row)\n",
        "        #print(counter_vect)\n",
        "        for item in row:\n",
        "            if item in vocab:\n",
        "                temp_loc= list_of_vocab.index(item) #get vocab_id of word\n",
        "                X_vec[i][temp_loc] = counter_vect[item]#assign count corresponding to vocabID\n",
        "        i+=1\n",
        "    return X_vec"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:40.219201Z",
          "start_time": "2020-02-15T14:17:40.215129Z"
        },
        "id": "pjOL2ga8Itk5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:27.950323Z",
          "iopub.execute_input": "2025-10-02T12:24:27.950655Z",
          "iopub.status.idle": "2025-10-02T12:24:27.956864Z",
          "shell.execute_reply.started": "2025-10-02T12:24:27.950629Z",
          "shell.execute_reply": "2025-10-02T12:24:27.955855Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Count vectors"
      ],
      "metadata": {
        "id": "dT4t9meMItk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_count = vectorise(X_ngram,vocab)\n",
        "print(X_tr_count)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:28.145788Z",
          "start_time": "2020-02-15T14:17:28.066100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER9lCbWHItk7",
        "outputId": "38fc5f88-9653-4b2f-de3b-d9b8a5105448",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:31.968177Z",
          "iopub.execute_input": "2025-10-02T12:24:31.968489Z",
          "iopub.status.idle": "2025-10-02T12:24:35.664298Z",
          "shell.execute_reply.started": "2025-10-02T12:24:31.968462Z",
          "shell.execute_reply": "2025-10-02T12:24:35.662966Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_count.shape #(1400, 5000)\n",
        "type(X_tr_count)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:42.004808Z",
          "start_time": "2020-02-15T14:17:42.001555Z"
        },
        "id": "B3QsyjK4Itk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:35.665996Z",
          "iopub.execute_input": "2025-10-02T12:24:35.666371Z",
          "iopub.status.idle": "2025-10-02T12:24:35.675494Z",
          "shell.execute_reply.started": "2025-10-02T12:24:35.666338Z",
          "shell.execute_reply": "2025-10-02T12:24:35.673690Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52f21c5-1a86-485a-f410-7dceb4b9b02a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_count[:2,:50]"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:17:42.010525Z",
          "start_time": "2020-02-15T14:17:42.006309Z"
        },
        "id": "22q2MPT6Itk8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:37.522163Z",
          "iopub.execute_input": "2025-10-02T12:24:37.522458Z",
          "iopub.status.idle": "2025-10-02T12:24:37.530561Z",
          "shell.execute_reply.started": "2025-10-02T12:24:37.522437Z",
          "shell.execute_reply": "2025-10-02T12:24:37.529384Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c1f5f2a-aee7-4d38-838a-9e3079a2eb00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Logistic Regression\n",
        "\n",
        "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
      ],
      "metadata": {
        "id": "9ju-zXZfItlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, you need to implement the `sigmoid` function. It takes as input:\n",
        "\n",
        "- `z`: a real number or an array of real numbers\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `sig`: the sigmoid of `z`"
      ],
      "metadata": {
        "id": "O_ttDud4ItlB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
        "\n",
        "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
        "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `preds_proba`: the prediction probabilities of X given the weights"
      ],
      "metadata": {
        "id": "3j1J3y2ZItlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
        "\n",
        "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
        "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `preds_class`: the predicted class for each x in X given the weights"
      ],
      "metadata": {
        "id": "DH3PbLAwItlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
        "\n",
        "- `X`: input vectors\n",
        "- `Y`: labels\n",
        "- `weights`: model weights\n",
        "- `alpha`: regularisation strength\n",
        "\n",
        "and return:\n",
        "\n",
        "- `l`: the loss score"
      ],
      "metadata": {
        "id": "SKxgHtEOItlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
        "\n",
        "- `X_tr`: array of training data (vectors)\n",
        "- `Y_tr`: labels of `X_tr`\n",
        "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
        "- `Y_dev`: labels of `X_dev`\n",
        "- `lr`: learning rate\n",
        "- `alpha`: regularisation strength\n",
        "- `epochs`: number of full passes over the training data\n",
        "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
        "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
        "\n",
        "\n",
        "and returns:\n",
        "\n",
        "- `weights`: the weights learned\n",
        "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
        "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
      ],
      "metadata": {
        "id": "TibY4gOUItlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate Logistic Regression with Count vectors\n",
        "\n",
        "First train the model using SGD:"
      ],
      "metadata": {
        "id": "A9z1AbenItlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Calculate X_dev_count & X_test_count\n",
        "XDev_ngram = Extract_X_ngram(data_dev_textlist)\n",
        "X_dev_count = vectorise(XDev_ngram,vocab)\n",
        "\n",
        "XTest_ngram = Extract_X_ngram(data_test_textlist)\n",
        "X_test_count = vectorise(XTest_ngram,vocab)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:45.673280Z",
          "iopub.execute_input": "2025-10-02T12:24:45.673617Z",
          "iopub.status.idle": "2025-10-02T12:24:47.182539Z",
          "shell.execute_reply.started": "2025-10-02T12:24:45.673574Z",
          "shell.execute_reply": "2025-10-02T12:24:47.181451Z"
        },
        "id": "j6kDnnbU531y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Huấn luyện Logistic Regression với sklearn\n",
        "clf = LogisticRegression(\n",
        "    # C=0.05,\n",
        "    # max_iter=1000,          # số vòng lặp tối đa\n",
        "    # solver=\"lbfgs\",         # bộ giải (tối ưu hóa) phổ biến\n",
        "    # multi_class=\"auto\"      # auto: binary hoặc multinomial đều chạy\n",
        ")\n",
        "clf.fit(X_tr_count, ArrayTR.ravel())  # train\n",
        "\n",
        "# Dự đoán trên tập validation/dev\n",
        "y_pred_dev = clf.predict(X_dev_count)\n",
        "\n",
        "print(\"Classification Report (Dev set):\")\n",
        "print(classification_report(ArrayDev, y_pred_dev))\n",
        "\n",
        "print(\"Confusion Matrix (Dev set):\")\n",
        "print(confusion_matrix(ArrayDev, y_pred_dev))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:48.532998Z",
          "iopub.execute_input": "2025-10-02T12:24:48.534366Z",
          "iopub.status.idle": "2025-10-02T12:24:51.442521Z",
          "shell.execute_reply.started": "2025-10-02T12:24:48.534319Z",
          "shell.execute_reply": "2025-10-02T12:24:51.441671Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri_xaktB531y",
        "outputId": "bf7658bb-77ab-40dd-dc89-ea0b185b5a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Dev set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.69      0.71       400\n",
            "           1       0.71      0.73      0.72       400\n",
            "\n",
            "    accuracy                           0.71       800\n",
            "   macro avg       0.71      0.71      0.71       800\n",
            "weighted avg       0.71      0.71      0.71       800\n",
            "\n",
            "Confusion Matrix (Dev set):\n",
            "[[278 122]\n",
            " [108 292]]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Câu mới nhập\n",
        "new_sentence = [\"best tweet ever!\"]\n",
        "\n",
        "# 2. Chuyển câu mới thành n-gram giống như khi train\n",
        "new_sentence_ngram = Extract_X_ngram(new_sentence)\n",
        "\n",
        "# 3. Vectorize bằng hàm thủ công\n",
        "X_new = vectorise(new_sentence_ngram, list_of_vocab)\n",
        "\n",
        "# 4. Dự đoán nhãn\n",
        "y_pred_new = clf.predict(X_new)\n",
        "\n",
        "print(\"🔮 Dự đoán nhãn cho câu:\", new_sentence[0])\n",
        "print(\"👉 Kết quả:\", y_pred_new[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:24:54.615912Z",
          "iopub.execute_input": "2025-10-02T12:24:54.616778Z",
          "iopub.status.idle": "2025-10-02T12:24:54.625436Z",
          "shell.execute_reply.started": "2025-10-02T12:24:54.616747Z",
          "shell.execute_reply": "2025-10-02T12:24:54.623865Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlQKETkb531z",
        "outputId": "f9e3f2ca-03a2-46f2-e9cd-2373f294da79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔮 Dự đoán nhãn cho câu: best tweet ever!\n",
            "👉 Kết quả: 1\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-class Logistic Regression\n",
        "\n",
        "Now you need to train a Multiclass Logistic Regression (MLR) Classifier by extending the Binary model you developed above. You will use the MLR model to perform topic classification on the AG news dataset consisting of three classes:"
      ],
      "metadata": {
        "id": "O6SJxnABItli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ======================\n",
        "# 1. Chọn categories\n",
        "# ======================\n",
        "categories = ['sci.med','rec.sport.hockey', 'sci.space', 'talk.religion.misc', 'comp.graphics']\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    remove=('headers','footers','quotes')\n",
        ")\n",
        "newsgroups_test = fetch_20newsgroups(\n",
        "    subset='test',\n",
        "    categories=categories,\n",
        "    remove=('headers','footers','quotes')\n",
        ")\n",
        "\n",
        "# Gộp train + test lại để chọn subset ~3000\n",
        "all_texts = newsgroups_train.data + newsgroups_test.data\n",
        "all_labels = list(newsgroups_train.target) + list(newsgroups_test.target)\n",
        "\n",
        "# ======================\n",
        "# 2. Lấy ngẫu nhiên 3000 samples\n",
        "# ======================\n",
        "subset_texts, _, subset_labels, _ = train_test_split(\n",
        "    all_texts,\n",
        "    all_labels,\n",
        "    train_size=4000,\n",
        "    stratify=all_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 3. Chia thành train/dev/test (70/15/15)\n",
        "# ======================\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    subset_texts, subset_labels,\n",
        "    test_size=0.3, random_state=42, stratify=subset_labels\n",
        ")\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# ======================\n",
        "# 4. Tạo DataFrame như code gốc\n",
        "# ======================\n",
        "Topic_data_tr = pd.DataFrame({\"label\": y_train, \"text\": X_train})\n",
        "Topic_data_dev = pd.DataFrame({\"label\": y_dev, \"text\": X_dev})\n",
        "Topic_data_test = pd.DataFrame({\"label\": y_test, \"text\": X_test})\n",
        "\n",
        "# ======================\n",
        "# 5. Đồng thời tạo list + numpy array (format cũ)\n",
        "# ======================\n",
        "topic_tr_textlist = Topic_data_tr[[\"text\"]].values.tolist()\n",
        "ArrayTR_Topic = Topic_data_tr[[\"label\"]].to_numpy()\n",
        "\n",
        "topic_dev_textlist = Topic_data_dev[[\"text\"]].values.tolist()\n",
        "ArrayDev_Topic = Topic_data_dev[[\"label\"]].to_numpy()\n",
        "\n",
        "topic_test_textlist = Topic_data_test[[\"text\"]].values.tolist()\n",
        "ArrayTest_Topic = Topic_data_test[[\"label\"]].to_numpy()\n",
        "\n",
        "# ======================\n",
        "# 6. Kiểm tra\n",
        "# ======================\n",
        "print(\"Train size:\", len(Topic_data_tr))\n",
        "print(\"Dev size:\", len(Topic_data_dev))\n",
        "print(\"Test size:\", len(Topic_data_test))\n",
        "print(\"Số lớp:\", len(np.unique(ArrayTR_Topic)))\n",
        "\n",
        "Topic_data_tr.head()\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:03.212229Z",
          "start_time": "2020-02-15T14:18:03.185261Z"
        },
        "id": "bNDJ6sX0Itlk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:03.761420Z",
          "iopub.execute_input": "2025-10-02T12:25:03.761759Z",
          "iopub.status.idle": "2025-10-02T12:25:05.993383Z",
          "shell.execute_reply.started": "2025-10-02T12:25:03.761735Z",
          "shell.execute_reply": "2025-10-02T12:25:05.992455Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "992a28d7-b097-4f28-e95f-9dcb5e48c839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 2800\n",
            "Dev size: 600\n",
            "Test size: 600\n",
            "Số lớp: 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                               text\n",
              "0      3  At one time there was speculation that the fir...\n",
              "1      0  I am looking for some fast polygon routines (S...\n",
              "2      1  Can some on e give me some stats on Forsrg in ...\n",
              "3      3  Regarding the feasability of retrieving the HS...\n",
              "4      1  -=> Quoting Greg Rogers to All <=-\\n GR> Hi al..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6eef03cb-d578-4952-96db-82d6eb730717\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>At one time there was speculation that the fir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I am looking for some fast polygon routines (S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Can some on e give me some stats on Forsrg in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Regarding the feasability of retrieving the HS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>-=&gt; Quoting Greg Rogers to All &lt;=-\\n GR&gt; Hi al...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6eef03cb-d578-4952-96db-82d6eb730717')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6eef03cb-d578-4952-96db-82d6eb730717 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6eef03cb-d578-4952-96db-82d6eb730717');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-75cf13cd-8883-41c9-9319-75b93e034c89\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-75cf13cd-8883-41c9-9319-75b93e034c89')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-75cf13cd-8883-41c9-9319-75b93e034c89 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Topic_data_tr",
              "summary": "{\n  \"name\": \"Topic_data_tr\",\n  \"rows\": 2800,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2726,\n        \"samples\": [\n          \"I have just started taking allergy shots a month ago and is \\nstill wondering what I am getting into. A friend of mine told\\nme that the body change every 7 years (whatever that means)\\nand I don't need those antibody-building allergy shots at all.\\nDoes that make sense to anyone?\",\n          \"Oops, what the hell a crosspost is this ?!\\n\\nHave a look onto XV-3.00 before saying anything more about it's power.\\n\",\n          \"#|> #|> \\n#|> #|> #This is quite different from saying \\\"Employing force on other people\\n#|> #|> #is immoral, period.   Unfortunately, from time to time we are obliged\\n#|> #|> #to do this immoral thing for reasons of self-preservation, and so\\n#|> #|> #we have to bear the moral consequences of that.\\n#|> #|> \\n#|> #|> Since both statements, to all intents and purposes, say effectively\\n#|> #|> the same thing, \\n#|> #\\n#|> #Are you serious?  Two statements, one of which says that use of force\\n#|> #in the given situation is moral, and the other of which says it is\\n#|> #not moral \\\"say effectively the same thing?\\\"\\n#|> \\n#|> Yes, when you tag on the \\\"Unfortunately, ...\\\", then to all intents and\\n#|> purposes you are saying the same thing.\\n#\\n#Then delete the \\\"unfortunately\\\".   Now tell me that the two statement\\n#say effectively the same thing.\\n#\\n#And to save everyone a couple of trips round this loop, please notice\\n#that we are only obliged to use force to preserve self.   We can choose\\n#*not* to preserve self, which is the point of pacifism.\\n\\nO.K., got you.  I concede your point, though the word \\\"obliged\\\" strongly\\nimplies that one must sometimes use force.  A further rephrasing would\\ngive you the distinction you mention, however.  If I have you right, a pacifist\\nwould not even go on to say, \\\"unfortunately,etc.\\\"\\n\\n#|> #Would you say this of any two statements, one saying \\\"X is moral\\\" and\\n#|> #the other saying \\\"X is immoral?\\\"   How would you decided when two \\n#|> #statements \\\"X is moral\\\" \\\"X is immoral\\\" actually conflict, and when\\n#|> #they \\\"say effectively the same thing\\\".\\n#|> \\n#|> What they prescribe that one should do is a pretty good indicator.\\n#\\n#And in this case they don't prescribe the same things, so.....\\n\\nYes, fair enough, though why confuse things by saying that \\\"one is \\nsomtimes obliged\\\" if the real meaning is that \\\"one is never obliged\\\".\\n\\n#|> #|>                  and lead one to do precisely the same thing, then \\n#|> #|> either both statements are doublespeak, or none.\\n#|> #\\n#|> #They might lead you to do the same thing, but the difference is what\\n#|> #motivates pacifism so they obviously don't lead pacifists to to the\\n#|> #same thing.\\n#|> \\n#|> That's not true.  You could formulate a pragmatic belief in minimum \\n#|> force and still be a pacifist.  If the minimum is 0, great  - but one is\\n#|> always trying to get as close to 0 force as possible under that belief.\\n#|> Not the same as 'force is immoral, period', but still tending to pacifism.\\n#\\n#If you don't think the use of force is immoral, why minimise its use?\\n\\nIf you don't think that it is \\\"immoral, period.\\\".   \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(topic_test_textlist))"
      ],
      "metadata": {
        "id": "rwV__06LItlk",
        "outputId": "f58f1128-4e57-4b7a-e9e3-77429fb0838f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:11.168380Z",
          "iopub.execute_input": "2025-10-02T12:25:11.168732Z",
          "iopub.status.idle": "2025-10-02T12:25:11.174150Z",
          "shell.execute_reply.started": "2025-10-02T12:25:11.168708Z",
          "shell.execute_reply": "2025-10-02T12:25:11.173061Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "600\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Topic_data_tr.head()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:03.515585Z",
          "start_time": "2020-02-15T14:18:03.508299Z"
        },
        "id": "K3PzDaTjItll",
        "outputId": "74447f08-c4fd-42de-cb63-62db4dc342e5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:12.910973Z",
          "iopub.execute_input": "2025-10-02T12:25:12.911302Z",
          "iopub.status.idle": "2025-10-02T12:25:12.919993Z",
          "shell.execute_reply.started": "2025-10-02T12:25:12.911277Z",
          "shell.execute_reply": "2025-10-02T12:25:12.919096Z"
        }
      },
      "outputs": [
        {
          "execution_count": 127,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   label                                               text\n0      3  At one time there was speculation that the fir...\n1      0  I am looking for some fast polygon routines (S...\n2      1  Can some on e give me some stats on Forsrg in ...\n3      3  Regarding the feasability of retrieving the HS...\n4      1  -=> Quoting Greg Rogers to All <=-\\n GR> Hi al...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>At one time there was speculation that the fir...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>I am looking for some fast polygon routines (S...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Can some on e give me some stats on Forsrg in ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Regarding the feasability of retrieving the HS...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>-=&gt; Quoting Greg Rogers to All &lt;=-\\n GR&gt; Hi al...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "topic_vocab, topic_df, topic_ngram_counts_tr = get_vocab(topic_tr_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
        "#topic_vocab, topic_df_dev, topic_ngram_counts_dev = get_vocab(topic_dev_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
        "#topic_vocab,topic_df_test,topic_ngram_counts_test = get_vocab(topic_test_textlist, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:03.806523Z",
          "start_time": "2020-02-15T14:18:03.798279Z"
        },
        "id": "PJxxIxHMItlm",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:14.963327Z",
          "iopub.execute_input": "2025-10-02T12:25:14.963670Z",
          "iopub.status.idle": "2025-10-02T12:25:37.740708Z",
          "shell.execute_reply.started": "2025-10-02T12:25:14.963644Z",
          "shell.execute_reply": "2025-10-02T12:25:37.739678Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_vocab = list(topic_vocab)\n",
        "vocab_id_topic = {i:list_of_vocab[i] for i in range(len(topic_vocab))}"
      ],
      "metadata": {
        "id": "xks459SjItln",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:43.386972Z",
          "iopub.execute_input": "2025-10-02T12:25:43.387297Z",
          "iopub.status.idle": "2025-10-02T12:25:43.393812Z",
          "shell.execute_reply.started": "2025-10-02T12:25:43.387271Z",
          "shell.execute_reply": "2025-10-02T12:25:43.392673Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(topic_vocab))\n",
        "print(list(topic_vocab)[:100])\n",
        "print()\n",
        "print(topic_df.most_common()[:10])\n",
        "#print(list_of_vocab[:10])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:04.508938Z",
          "start_time": "2020-02-15T14:18:04.171071Z"
        },
        "scrolled": true,
        "id": "v-x8qDnWItlw",
        "outputId": "f043e37f-5cc5-4c38-c51c-181ff4facdb5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:46.139875Z",
          "iopub.execute_input": "2025-10-02T12:25:46.140226Z",
          "iopub.status.idle": "2025-10-02T12:25:46.256760Z",
          "shell.execute_reply.started": "2025-10-02T12:25:46.140200Z",
          "shell.execute_reply": "2025-10-02T12:25:46.255685Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "5000\n['quiet', 'diagnosis', 'HELP', 'unc', 'newly', 'even though', 'Suite', 'He', 'nGeorge', 'circle', 'bother', 'meet', 've heard', 'unusual', 'OF', 'nChris', 'okay', 'nI agree', 'craft', 'nDetroit', 'Analysis', 'controlled', 'Centre', 'NYR', 'Young', 'studied', 'massive', 'shameful surrender too', 'resolution', 'combination', 'capabilities', 'npoint', 'wing', 'countries', 'requests', 'difference between', 'library', 'Bay', 'justify', 'hours', 'forever', 'one another', 'involves', 'format', 'engineers', 'single', 'nThanks', 'ON', 'non', 'centers', 'nBy', 'Canadiens', 'acts', 'schools', 'nalso', 'ESPN', 'landing', 'Central', 'last night', 'cult', 'occurred', 'all sorts', 'might', 'farm', 'York', 'magazine', 'effective', 'classic', 'guidelines', 'launched', 'edu au', 'wondering', 'funding', 'bad', 'function', 'delivery', 'America', 'impact', 'powerful', 'heat', 'prevention', 'Hope', 'arbitrary', 'offering', 'heart', 'cl msu', 'nmaybe', 'voice', 'attached', 'heavily', 'would say', 'versus', 'via mail', 'uk', 'bearing', 'Mission', 'dates', 'nDon', 'message', 'but ve']\n\n[('not', 1029), ('but', 960), ('would', 776), ('if', 764), ('about', 744), ('nI', 730), ('one', 649), ('all', 624), ('some', 614), ('out', 613)]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "##Counts\n",
        "\n",
        "topic_X_tr_ngram = Extract_X_ngram(topic_tr_textlist)\n",
        "topic_X_tr_count = vectorise(topic_X_tr_ngram,topic_vocab)\n",
        "\n",
        "topic_XDev_ngram = Extract_X_ngram(topic_dev_textlist)\n",
        "topic_X_dev_count = vectorise(topic_XDev_ngram,topic_vocab)\n",
        "\n",
        "topic_XTest_ngram = Extract_X_ngram(topic_test_textlist)\n",
        "topic_X_test_count = vectorise(topic_XTest_ngram,topic_vocab)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-02-15T14:18:04.706802Z",
          "start_time": "2020-02-15T14:18:04.511061Z"
        },
        "id": "RnBQSkOaItlx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:25:48.693181Z",
          "iopub.execute_input": "2025-10-02T12:25:48.693483Z",
          "iopub.status.idle": "2025-10-02T12:26:34.492833Z",
          "shell.execute_reply.started": "2025-10-02T12:25:48.693462Z",
          "shell.execute_reply": "2025-10-02T12:26:34.491698Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ======================\n",
        "# 1. Train với Count Vectors\n",
        "# ======================\n",
        "sgd_count = SGDClassifier(\n",
        "    loss=\"log_loss\",      # logistic regression\n",
        "    penalty=\"l2\",         # regularization\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "sgd_count.fit(topic_X_tr_count, ArrayTR_Topic.ravel())\n",
        "\n",
        "# Evaluate trên Dev set\n",
        "y_dev_pred_count = sgd_count.predict(topic_X_dev_count)\n",
        "print(\"=== Logistic Regression (Count Vectors) ===\")\n",
        "print(classification_report(ArrayDev_Topic, y_dev_pred_count))\n",
        "print(confusion_matrix(ArrayDev_Topic, y_dev_pred_count))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:28:07.593311Z",
          "iopub.execute_input": "2025-10-02T12:28:07.593669Z",
          "iopub.status.idle": "2025-10-02T12:28:10.685209Z",
          "shell.execute_reply.started": "2025-10-02T12:28:07.593643Z",
          "shell.execute_reply": "2025-10-02T12:28:10.683958Z"
        },
        "id": "B9RRUwxa5312",
        "outputId": "d94fac96-1c30-48e6-fe74-f1f1dc7f4f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "=== Logistic Regression (Count Vectors) ===\n              precision    recall  f1-score   support\n\n           0       0.84      0.87      0.85       127\n           1       0.88      0.85      0.87       131\n           2       0.78      0.85      0.81       130\n           3       0.84      0.75      0.79       129\n           4       0.74      0.76      0.75        83\n\n    accuracy                           0.82       600\n   macro avg       0.82      0.82      0.81       600\nweighted avg       0.82      0.82      0.82       600\n\n[[110   3  10   2   2]\n [  1 112   6   5   7]\n [  9   0 110   6   5]\n [  9   4  11  97   8]\n [  2   8   4   6  63]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Input mới\n",
        "new_sentence = [\"Astronomers discovered a new exoplanet orbiting a distant star, raising hopes for extraterrestrial life\"]\n",
        "\n",
        "# 2. N-gram hóa (giống lúc train)\n",
        "new_sentence_ngram = Extract_X_ngram(new_sentence)\n",
        "\n",
        "# 3. Vectorize với vocab cũ\n",
        "X_new_count = vectorise(new_sentence_ngram, list_of_vocab)  # Count vector\n",
        "\n",
        "# 4. Predict với mô hình Count\n",
        "y_pred_new_count = sgd_count.predict(X_new_count)\n",
        "\n",
        "print(\"🔮 Câu:\", new_sentence[0])\n",
        "print(\"👉 Dự đoán (Count):\", y_pred_new_count[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:28:17.231421Z",
          "iopub.execute_input": "2025-10-02T12:28:17.231767Z",
          "iopub.status.idle": "2025-10-02T12:28:17.242361Z",
          "shell.execute_reply.started": "2025-10-02T12:28:17.231743Z",
          "shell.execute_reply": "2025-10-02T12:28:17.241075Z"
        },
        "id": "FlNmVnKx5313",
        "outputId": "584f72ba-6b76-468d-df5d-554bd5380030"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "🔮 Câu: Astronomers discovered a new exoplanet orbiting a distant star, raising hopes for extraterrestrial life\n👉 Dự đoán (Count): 3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you need to compute the categorical cross entropy loss (extending the binary loss to support multiple classes)."
      ],
      "metadata": {
        "id": "4DQAwChkItl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lấy lại categories bạn dùng khi load dataset\n",
        "categories = ['sci.med','rec.sport.hockey', 'sci.space', 'talk.religion.misc', 'comp.graphics']\n",
        "\n",
        "# Load dataset train\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    remove=('headers','footers','quotes')\n",
        ")\n",
        "\n",
        "# In các class số\n",
        "print(\"Các class số:\", np.unique(newsgroups_train.target))\n",
        "\n",
        "# In mapping class số -> label\n",
        "for i, name in enumerate(newsgroups_train.target_names):\n",
        "    print(f\"{i} → {name}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-02T12:28:19.969072Z",
          "iopub.execute_input": "2025-10-02T12:28:19.969372Z",
          "iopub.status.idle": "2025-10-02T12:28:21.299407Z",
          "shell.execute_reply.started": "2025-10-02T12:28:19.969351Z",
          "shell.execute_reply": "2025-10-02T12:28:21.298540Z"
        },
        "id": "CF4NjYMF5313",
        "outputId": "291423d1-620a-4ba8-e772-380c706ccd27"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Các class số: [0 1 2 3 4]\n0 → comp.graphics\n1 → rec.sport.hockey\n2 → sci.med\n3 → sci.space\n4 → talk.religion.misc\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}